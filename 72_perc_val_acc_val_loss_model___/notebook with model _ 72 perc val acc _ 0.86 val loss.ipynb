{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# # This Python 3 environment comes with many helpful analytics libraries installed\n","# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# # For example, here's several helpful packages to load\n","\n","# import numpy as np # linear algebra\n","# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# # Input data files are available in the read-only \"../input/\" directory\n","# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","# import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T01:45:55.194908Z","iopub.status.busy":"2023-12-06T01:45:55.194509Z","iopub.status.idle":"2023-12-06T01:45:55.200707Z","shell.execute_reply":"2023-12-06T01:45:55.199455Z","shell.execute_reply.started":"2023-12-06T01:45:55.194848Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","\n","import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T01:45:55.204269Z","iopub.status.busy":"2023-12-06T01:45:55.203916Z","iopub.status.idle":"2023-12-06T01:45:55.431945Z","shell.execute_reply":"2023-12-06T01:45:55.430786Z","shell.execute_reply.started":"2023-12-06T01:45:55.204232Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T04:36:54.901370Z","iopub.status.busy":"2023-12-06T04:36:54.900929Z","iopub.status.idle":"2023-12-06T05:10:47.599373Z","shell.execute_reply":"2023-12-06T05:10:47.598245Z","shell.execute_reply.started":"2023-12-06T04:36:54.901336Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 7000 files belonging to 7 classes.\n","Using 5600 files for training.\n","Found 7000 files belonging to 7 classes.\n","Using 1400 files for validation.\n","['chicken_wings', 'fish_and_chips', 'french_fries', 'hamburger', 'ice_cream', 'onion_rings', 'pizza']\n","Epoch 1/300\n","350/350 [==============================] - ETA: 0s - loss: 1.8604 - accuracy: 0.2288\n","Epoch 1: val_accuracy improved from -inf to 0.25000, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 1: val_loss improved from inf to 1.81652, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 20s 44ms/step - loss: 1.8604 - accuracy: 0.2288 - val_loss: 1.8165 - val_accuracy: 0.2500\n","Epoch 2/300\n","348/350 [============================>.] - ETA: 0s - loss: 1.8088 - accuracy: 0.2570\n","Epoch 2: val_accuracy improved from 0.25000 to 0.25847, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 2: val_loss improved from 1.81652 to 1.77928, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 13s 36ms/step - loss: 1.8096 - accuracy: 0.2566 - val_loss: 1.7793 - val_accuracy: 0.2585\n","Epoch 3/300\n","347/350 [============================>.] - ETA: 0s - loss: 1.7755 - accuracy: 0.2822\n","Epoch 3: val_accuracy improved from 0.25847 to 0.29025, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 3: val_loss improved from 1.77928 to 1.74486, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 13s 38ms/step - loss: 1.7751 - accuracy: 0.2827 - val_loss: 1.7449 - val_accuracy: 0.2903\n","Epoch 4/300\n","349/350 [============================>.] - ETA: 0s - loss: 1.7483 - accuracy: 0.3030\n","Epoch 4: val_accuracy improved from 0.29025 to 0.29873, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 4: val_loss improved from 1.74486 to 1.71775, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 13s 36ms/step - loss: 1.7483 - accuracy: 0.3032 - val_loss: 1.7178 - val_accuracy: 0.2987\n","Epoch 5/300\n","347/350 [============================>.] - ETA: 0s - loss: 1.6984 - accuracy: 0.3278\n","Epoch 5: val_accuracy improved from 0.29873 to 0.35805, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 5: val_loss improved from 1.71775 to 1.65237, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 13s 38ms/step - loss: 1.6988 - accuracy: 0.3275 - val_loss: 1.6524 - val_accuracy: 0.3581\n","Epoch 6/300\n","350/350 [==============================] - ETA: 0s - loss: 1.6384 - accuracy: 0.3620\n","Epoch 6: val_accuracy improved from 0.35805 to 0.43432, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 6: val_loss improved from 1.65237 to 1.51166, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 13s 36ms/step - loss: 1.6384 - accuracy: 0.3620 - val_loss: 1.5117 - val_accuracy: 0.4343\n","Epoch 7/300\n","350/350 [==============================] - ETA: 0s - loss: 1.5926 - accuracy: 0.3934\n","Epoch 7: val_accuracy did not improve from 0.43432\n","\n","Epoch 7: val_loss improved from 1.51166 to 1.45349, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 10s 27ms/step - loss: 1.5926 - accuracy: 0.3934 - val_loss: 1.4535 - val_accuracy: 0.4301\n","Epoch 8/300\n","350/350 [==============================] - ETA: 0s - loss: 1.5385 - accuracy: 0.4175\n","Epoch 8: val_accuracy improved from 0.43432 to 0.47881, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 8: val_loss improved from 1.45349 to 1.40376, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 13s 36ms/step - loss: 1.5385 - accuracy: 0.4175 - val_loss: 1.4038 - val_accuracy: 0.4788\n","Epoch 9/300\n","349/350 [============================>.] - ETA: 0s - loss: 1.5169 - accuracy: 0.4239\n","Epoch 9: val_accuracy did not improve from 0.47881\n","\n","Epoch 9: val_loss did not improve from 1.40376\n","350/350 [==============================] - 6s 17ms/step - loss: 1.5170 - accuracy: 0.4238 - val_loss: 1.4294 - val_accuracy: 0.4555\n","Epoch 10/300\n","347/350 [============================>.] - ETA: 0s - loss: 1.4649 - accuracy: 0.4611\n","Epoch 10: val_accuracy improved from 0.47881 to 0.48093, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 10: val_loss improved from 1.40376 to 1.39875, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 14s 39ms/step - loss: 1.4648 - accuracy: 0.4620 - val_loss: 1.3987 - val_accuracy: 0.4809\n","Epoch 11/300\n","349/350 [============================>.] - ETA: 0s - loss: 1.4276 - accuracy: 0.4626\n","Epoch 11: val_accuracy improved from 0.48093 to 0.54237, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 11: val_loss improved from 1.39875 to 1.28047, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 13s 36ms/step - loss: 1.4276 - accuracy: 0.4629 - val_loss: 1.2805 - val_accuracy: 0.5424\n","Epoch 12/300\n","347/350 [============================>.] - ETA: 0s - loss: 1.3755 - accuracy: 0.4960\n","Epoch 12: val_accuracy did not improve from 0.54237\n","\n","Epoch 12: val_loss did not improve from 1.28047\n","350/350 [==============================] - 6s 17ms/step - loss: 1.3762 - accuracy: 0.4963 - val_loss: 1.3790 - val_accuracy: 0.5106\n","Epoch 13/300\n","350/350 [==============================] - ETA: 0s - loss: 1.3683 - accuracy: 0.5013\n","Epoch 13: val_accuracy did not improve from 0.54237\n","\n","Epoch 13: val_loss improved from 1.28047 to 1.26027, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 9s 26ms/step - loss: 1.3683 - accuracy: 0.5013 - val_loss: 1.2603 - val_accuracy: 0.5297\n","Epoch 14/300\n","348/350 [============================>.] - ETA: 0s - loss: 1.3305 - accuracy: 0.5057\n","Epoch 14: val_accuracy improved from 0.54237 to 0.56356, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 14: val_loss did not improve from 1.26027\n","350/350 [==============================] - 9s 27ms/step - loss: 1.3307 - accuracy: 0.5057 - val_loss: 1.2729 - val_accuracy: 0.5636\n","Epoch 15/300\n","348/350 [============================>.] - ETA: 0s - loss: 1.2803 - accuracy: 0.5296\n","Epoch 15: val_accuracy did not improve from 0.56356\n","\n","Epoch 15: val_loss improved from 1.26027 to 1.21042, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 9s 27ms/step - loss: 1.2822 - accuracy: 0.5289 - val_loss: 1.2104 - val_accuracy: 0.5508\n","Epoch 16/300\n","350/350 [==============================] - ETA: 0s - loss: 1.2807 - accuracy: 0.5286\n","Epoch 16: val_accuracy did not improve from 0.56356\n","\n","Epoch 16: val_loss improved from 1.21042 to 1.20235, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 10s 30ms/step - loss: 1.2807 - accuracy: 0.5286 - val_loss: 1.2024 - val_accuracy: 0.5636\n","Epoch 17/300\n","348/350 [============================>.] - ETA: 0s - loss: 1.2412 - accuracy: 0.5431\n","Epoch 17: val_accuracy did not improve from 0.56356\n","\n","Epoch 17: val_loss did not improve from 1.20235\n","350/350 [==============================] - 6s 18ms/step - loss: 1.2414 - accuracy: 0.5430 - val_loss: 1.2246 - val_accuracy: 0.5508\n","Epoch 18/300\n","348/350 [============================>.] - ETA: 0s - loss: 1.2352 - accuracy: 0.5393\n","Epoch 18: val_accuracy improved from 0.56356 to 0.56568, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 18: val_loss did not improve from 1.20235\n","350/350 [==============================] - 9s 27ms/step - loss: 1.2358 - accuracy: 0.5395 - val_loss: 1.2336 - val_accuracy: 0.5657\n","Epoch 19/300\n","347/350 [============================>.] - ETA: 0s - loss: 1.2080 - accuracy: 0.5564\n","Epoch 19: val_accuracy did not improve from 0.56568\n","\n","Epoch 19: val_loss did not improve from 1.20235\n","350/350 [==============================] - 6s 18ms/step - loss: 1.2097 - accuracy: 0.5559 - val_loss: 1.2212 - val_accuracy: 0.5339\n","Epoch 20/300\n","349/350 [============================>.] - ETA: 0s - loss: 1.1907 - accuracy: 0.5652\n","Epoch 20: val_accuracy did not improve from 0.56568\n","\n","Epoch 20: val_loss did not improve from 1.20235\n","350/350 [==============================] - 6s 17ms/step - loss: 1.1922 - accuracy: 0.5650 - val_loss: 1.3522 - val_accuracy: 0.5106\n","Epoch 21/300\n","349/350 [============================>.] - ETA: 0s - loss: 1.1817 - accuracy: 0.5621\n","Epoch 21: val_accuracy improved from 0.56568 to 0.57627, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 21: val_loss improved from 1.20235 to 1.16806, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 13s 36ms/step - loss: 1.1822 - accuracy: 0.5620 - val_loss: 1.1681 - val_accuracy: 0.5763\n","Epoch 22/300\n","349/350 [============================>.] - ETA: 0s - loss: 1.1678 - accuracy: 0.5795\n","Epoch 22: val_accuracy improved from 0.57627 to 0.59534, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 22: val_loss did not improve from 1.16806\n","350/350 [==============================] - 9s 27ms/step - loss: 1.1674 - accuracy: 0.5796 - val_loss: 1.2034 - val_accuracy: 0.5953\n","Epoch 23/300\n","350/350 [==============================] - ETA: 0s - loss: 1.1553 - accuracy: 0.5845\n","Epoch 23: val_accuracy did not improve from 0.59534\n","\n","Epoch 23: val_loss improved from 1.16806 to 1.16638, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 9s 27ms/step - loss: 1.1553 - accuracy: 0.5845 - val_loss: 1.1664 - val_accuracy: 0.5953\n","Epoch 24/300\n","350/350 [==============================] - ETA: 0s - loss: 1.1476 - accuracy: 0.5814\n","Epoch 24: val_accuracy did not improve from 0.59534\n","\n","Epoch 24: val_loss did not improve from 1.16638\n","350/350 [==============================] - 6s 18ms/step - loss: 1.1476 - accuracy: 0.5814 - val_loss: 1.1891 - val_accuracy: 0.5742\n","Epoch 25/300\n","350/350 [==============================] - ETA: 0s - loss: 1.0999 - accuracy: 0.5948\n","Epoch 25: val_accuracy did not improve from 0.59534\n","\n","Epoch 25: val_loss did not improve from 1.16638\n","350/350 [==============================] - 6s 17ms/step - loss: 1.0999 - accuracy: 0.5948 - val_loss: 1.1743 - val_accuracy: 0.5678\n","Epoch 26/300\n","349/350 [============================>.] - ETA: 0s - loss: 1.0960 - accuracy: 0.5976\n","Epoch 26: val_accuracy did not improve from 0.59534\n","\n","Epoch 26: val_loss did not improve from 1.16638\n","350/350 [==============================] - 6s 17ms/step - loss: 1.0971 - accuracy: 0.5970 - val_loss: 1.2703 - val_accuracy: 0.5699\n","Epoch 27/300\n","347/350 [============================>.] - ETA: 0s - loss: 1.0919 - accuracy: 0.6057\n","Epoch 27: val_accuracy improved from 0.59534 to 0.61441, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 27: val_loss did not improve from 1.16638\n","350/350 [==============================] - 9s 26ms/step - loss: 1.0937 - accuracy: 0.6052 - val_loss: 1.1793 - val_accuracy: 0.6144\n","Epoch 28/300\n","348/350 [============================>.] - ETA: 0s - loss: 1.0732 - accuracy: 0.6155\n","Epoch 28: val_accuracy did not improve from 0.61441\n","\n","Epoch 28: val_loss did not improve from 1.16638\n","350/350 [==============================] - 6s 18ms/step - loss: 1.0740 - accuracy: 0.6154 - val_loss: 1.1773 - val_accuracy: 0.5869\n","Epoch 29/300\n","349/350 [============================>.] - ETA: 0s - loss: 1.0841 - accuracy: 0.6098\n","Epoch 29: val_accuracy improved from 0.61441 to 0.64619, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 29: val_loss improved from 1.16638 to 1.09780, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 14s 40ms/step - loss: 1.0851 - accuracy: 0.6093 - val_loss: 1.0978 - val_accuracy: 0.6462\n","Epoch 30/300\n","349/350 [============================>.] - ETA: 0s - loss: 1.0733 - accuracy: 0.6044\n","Epoch 30: val_accuracy did not improve from 0.64619\n","\n","Epoch 30: val_loss improved from 1.09780 to 1.07393, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 10s 27ms/step - loss: 1.0746 - accuracy: 0.6043 - val_loss: 1.0739 - val_accuracy: 0.6186\n","Epoch 31/300\n","349/350 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.6316\n","Epoch 31: val_accuracy did not improve from 0.64619\n","\n","Epoch 31: val_loss did not improve from 1.07393\n","350/350 [==============================] - 6s 17ms/step - loss: 1.0350 - accuracy: 0.6316 - val_loss: 1.1484 - val_accuracy: 0.6144\n","Epoch 32/300\n","348/350 [============================>.] - ETA: 0s - loss: 1.0581 - accuracy: 0.6185\n","Epoch 32: val_accuracy did not improve from 0.64619\n","\n","Epoch 32: val_loss did not improve from 1.07393\n","350/350 [==============================] - 6s 17ms/step - loss: 1.0607 - accuracy: 0.6179 - val_loss: 1.1104 - val_accuracy: 0.6165\n","Epoch 33/300\n","350/350 [==============================] - ETA: 0s - loss: 1.0496 - accuracy: 0.6205\n","Epoch 33: val_accuracy did not improve from 0.64619\n","\n","Epoch 33: val_loss did not improve from 1.07393\n","350/350 [==============================] - 6s 17ms/step - loss: 1.0496 - accuracy: 0.6205 - val_loss: 1.0907 - val_accuracy: 0.6441\n","Epoch 34/300\n","350/350 [==============================] - ETA: 0s - loss: 1.0335 - accuracy: 0.6277\n","Epoch 34: val_accuracy did not improve from 0.64619\n","\n","Epoch 34: val_loss did not improve from 1.07393\n","350/350 [==============================] - 6s 17ms/step - loss: 1.0335 - accuracy: 0.6277 - val_loss: 1.1737 - val_accuracy: 0.6102\n","Epoch 35/300\n","350/350 [==============================] - ETA: 0s - loss: 1.0028 - accuracy: 0.6320\n","Epoch 35: val_accuracy did not improve from 0.64619\n","\n","Epoch 35: val_loss did not improve from 1.07393\n","350/350 [==============================] - 6s 17ms/step - loss: 1.0028 - accuracy: 0.6320 - val_loss: 1.2247 - val_accuracy: 0.5869\n","Epoch 36/300\n","350/350 [==============================] - ETA: 0s - loss: 1.0086 - accuracy: 0.6371\n","Epoch 36: val_accuracy did not improve from 0.64619\n","\n","Epoch 36: val_loss improved from 1.07393 to 1.04797, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 9s 27ms/step - loss: 1.0086 - accuracy: 0.6371 - val_loss: 1.0480 - val_accuracy: 0.6356\n","Epoch 37/300\n","350/350 [==============================] - ETA: 0s - loss: 1.0047 - accuracy: 0.6418\n","Epoch 37: val_accuracy did not improve from 0.64619\n","\n","Epoch 37: val_loss did not improve from 1.04797\n","350/350 [==============================] - 6s 18ms/step - loss: 1.0047 - accuracy: 0.6418 - val_loss: 1.2284 - val_accuracy: 0.5636\n","Epoch 38/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.9945 - accuracy: 0.6426\n","Epoch 38: val_accuracy did not improve from 0.64619\n","\n","Epoch 38: val_loss did not improve from 1.04797\n","350/350 [==============================] - 6s 17ms/step - loss: 0.9943 - accuracy: 0.6429 - val_loss: 1.0577 - val_accuracy: 0.6441\n","Epoch 39/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.9754 - accuracy: 0.6557\n","Epoch 39: val_accuracy did not improve from 0.64619\n","\n","Epoch 39: val_loss did not improve from 1.04797\n","350/350 [==============================] - 6s 17ms/step - loss: 0.9760 - accuracy: 0.6552 - val_loss: 1.2892 - val_accuracy: 0.5869\n","Epoch 40/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.9766 - accuracy: 0.6507\n","Epoch 40: val_accuracy improved from 0.64619 to 0.64831, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 40: val_loss did not improve from 1.04797\n","350/350 [==============================] - 9s 27ms/step - loss: 0.9782 - accuracy: 0.6511 - val_loss: 1.0689 - val_accuracy: 0.6483\n","Epoch 41/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.9728 - accuracy: 0.6494\n","Epoch 41: val_accuracy did not improve from 0.64831\n","\n","Epoch 41: val_loss did not improve from 1.04797\n","350/350 [==============================] - 6s 18ms/step - loss: 0.9722 - accuracy: 0.6496 - val_loss: 1.1100 - val_accuracy: 0.6314\n","Epoch 42/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.9435 - accuracy: 0.6597\n","Epoch 42: val_accuracy did not improve from 0.64831\n","\n","Epoch 42: val_loss did not improve from 1.04797\n","350/350 [==============================] - 6s 17ms/step - loss: 0.9444 - accuracy: 0.6596 - val_loss: 1.0900 - val_accuracy: 0.6462\n","Epoch 43/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.9467 - accuracy: 0.6622\n","Epoch 43: val_accuracy improved from 0.64831 to 0.66525, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 43: val_loss improved from 1.04797 to 0.99045, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 13s 36ms/step - loss: 0.9465 - accuracy: 0.6618 - val_loss: 0.9905 - val_accuracy: 0.6653\n","Epoch 44/300\n","350/350 [==============================] - ETA: 0s - loss: 0.9428 - accuracy: 0.6564\n","Epoch 44: val_accuracy did not improve from 0.66525\n","\n","Epoch 44: val_loss did not improve from 0.99045\n","350/350 [==============================] - 6s 17ms/step - loss: 0.9428 - accuracy: 0.6564 - val_loss: 1.0859 - val_accuracy: 0.6483\n","Epoch 45/300\n","350/350 [==============================] - ETA: 0s - loss: 0.9428 - accuracy: 0.6611\n","Epoch 45: val_accuracy did not improve from 0.66525\n","\n","Epoch 45: val_loss did not improve from 0.99045\n","350/350 [==============================] - 6s 17ms/step - loss: 0.9428 - accuracy: 0.6611 - val_loss: 1.1864 - val_accuracy: 0.6165\n","Epoch 46/300\n","350/350 [==============================] - ETA: 0s - loss: 0.9403 - accuracy: 0.6641\n","Epoch 46: val_accuracy did not improve from 0.66525\n","\n","Epoch 46: val_loss did not improve from 0.99045\n","350/350 [==============================] - 6s 17ms/step - loss: 0.9403 - accuracy: 0.6641 - val_loss: 1.0550 - val_accuracy: 0.6398\n","Epoch 47/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.9248 - accuracy: 0.6677\n","Epoch 47: val_accuracy did not improve from 0.66525\n","\n","Epoch 47: val_loss did not improve from 0.99045\n","350/350 [==============================] - 6s 17ms/step - loss: 0.9249 - accuracy: 0.6680 - val_loss: 1.0675 - val_accuracy: 0.6589\n","Epoch 48/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.9341 - accuracy: 0.6610\n","Epoch 48: val_accuracy did not improve from 0.66525\n","\n","Epoch 48: val_loss did not improve from 0.99045\n","350/350 [==============================] - 6s 17ms/step - loss: 0.9341 - accuracy: 0.6612 - val_loss: 1.1377 - val_accuracy: 0.6525\n","Epoch 49/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.9229 - accuracy: 0.6720\n","Epoch 49: val_accuracy did not improve from 0.66525\n","\n","Epoch 49: val_loss did not improve from 0.99045\n","350/350 [==============================] - 6s 17ms/step - loss: 0.9227 - accuracy: 0.6723 - val_loss: 1.0687 - val_accuracy: 0.6547\n","Epoch 50/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.9023 - accuracy: 0.6839\n","Epoch 50: val_accuracy did not improve from 0.66525\n","\n","Epoch 50: val_loss did not improve from 0.99045\n","350/350 [==============================] - 6s 17ms/step - loss: 0.9038 - accuracy: 0.6836 - val_loss: 1.1593 - val_accuracy: 0.6398\n","Epoch 51/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.9055 - accuracy: 0.6747\n","Epoch 51: val_accuracy improved from 0.66525 to 0.68008, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 51: val_loss did not improve from 0.99045\n","350/350 [==============================] - 9s 27ms/step - loss: 0.9061 - accuracy: 0.6752 - val_loss: 1.0065 - val_accuracy: 0.6801\n","Epoch 52/300\n","350/350 [==============================] - ETA: 0s - loss: 0.8919 - accuracy: 0.6818\n","Epoch 52: val_accuracy did not improve from 0.68008\n","\n","Epoch 52: val_loss did not improve from 0.99045\n","350/350 [==============================] - 6s 17ms/step - loss: 0.8919 - accuracy: 0.6818 - val_loss: 1.1897 - val_accuracy: 0.6462\n","Epoch 53/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.8932 - accuracy: 0.6798\n","Epoch 53: val_accuracy did not improve from 0.68008\n","\n","Epoch 53: val_loss did not improve from 0.99045\n","350/350 [==============================] - 6s 17ms/step - loss: 0.8928 - accuracy: 0.6802 - val_loss: 1.2750 - val_accuracy: 0.6165\n","Epoch 54/300\n","350/350 [==============================] - ETA: 0s - loss: 0.8920 - accuracy: 0.6855\n","Epoch 54: val_accuracy did not improve from 0.68008\n","\n","Epoch 54: val_loss did not improve from 0.99045\n","350/350 [==============================] - 6s 17ms/step - loss: 0.8920 - accuracy: 0.6855 - val_loss: 1.1178 - val_accuracy: 0.6208\n","Epoch 55/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.9029 - accuracy: 0.6812\n","Epoch 55: val_accuracy did not improve from 0.68008\n","\n","Epoch 55: val_loss did not improve from 0.99045\n","350/350 [==============================] - 6s 17ms/step - loss: 0.9026 - accuracy: 0.6814 - val_loss: 1.1965 - val_accuracy: 0.6123\n","Epoch 56/300\n","350/350 [==============================] - ETA: 0s - loss: 0.9055 - accuracy: 0.6777\n","Epoch 56: val_accuracy did not improve from 0.68008\n","\n","Epoch 56: val_loss did not improve from 0.99045\n","350/350 [==============================] - 6s 17ms/step - loss: 0.9055 - accuracy: 0.6777 - val_loss: 1.1704 - val_accuracy: 0.6292\n","Epoch 57/300\n","350/350 [==============================] - ETA: 0s - loss: 0.8819 - accuracy: 0.6889\n","Epoch 57: val_accuracy did not improve from 0.68008\n","\n","Epoch 57: val_loss did not improve from 0.99045\n","350/350 [==============================] - 6s 17ms/step - loss: 0.8819 - accuracy: 0.6889 - val_loss: 1.1577 - val_accuracy: 0.6441\n","Epoch 58/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.8788 - accuracy: 0.6941\n","Epoch 58: val_accuracy did not improve from 0.68008\n","\n","Epoch 58: val_loss did not improve from 0.99045\n","350/350 [==============================] - 6s 17ms/step - loss: 0.8816 - accuracy: 0.6936 - val_loss: 1.1005 - val_accuracy: 0.6462\n","Epoch 59/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.8643 - accuracy: 0.6943\n","Epoch 59: val_accuracy did not improve from 0.68008\n","\n","Epoch 59: val_loss did not improve from 0.99045\n","350/350 [==============================] - 6s 17ms/step - loss: 0.8648 - accuracy: 0.6941 - val_loss: 1.1089 - val_accuracy: 0.6462\n","Epoch 60/300\n","350/350 [==============================] - ETA: 0s - loss: 0.8623 - accuracy: 0.6927\n","Epoch 60: val_accuracy did not improve from 0.68008\n","\n","Epoch 60: val_loss did not improve from 0.99045\n","350/350 [==============================] - 6s 18ms/step - loss: 0.8623 - accuracy: 0.6927 - val_loss: 1.0331 - val_accuracy: 0.6737\n","Epoch 61/300\n","350/350 [==============================] - ETA: 0s - loss: 0.8643 - accuracy: 0.6957\n","Epoch 61: val_accuracy did not improve from 0.68008\n","\n","Epoch 61: val_loss improved from 0.99045 to 0.95968, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 10s 27ms/step - loss: 0.8643 - accuracy: 0.6957 - val_loss: 0.9597 - val_accuracy: 0.6631\n","Epoch 62/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.8489 - accuracy: 0.7004\n","Epoch 62: val_accuracy did not improve from 0.68008\n","\n","Epoch 62: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 18ms/step - loss: 0.8504 - accuracy: 0.7000 - val_loss: 1.0152 - val_accuracy: 0.6610\n","Epoch 63/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.8787 - accuracy: 0.6862\n","Epoch 63: val_accuracy did not improve from 0.68008\n","\n","Epoch 63: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.8804 - accuracy: 0.6855 - val_loss: 1.2117 - val_accuracy: 0.6165\n","Epoch 64/300\n","350/350 [==============================] - ETA: 0s - loss: 0.8506 - accuracy: 0.7030\n","Epoch 64: val_accuracy did not improve from 0.68008\n","\n","Epoch 64: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.8506 - accuracy: 0.7030 - val_loss: 1.0627 - val_accuracy: 0.6398\n","Epoch 65/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.8385 - accuracy: 0.7058\n","Epoch 65: val_accuracy did not improve from 0.68008\n","\n","Epoch 65: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.8386 - accuracy: 0.7057 - val_loss: 1.2009 - val_accuracy: 0.6504\n","Epoch 66/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.8245 - accuracy: 0.7097\n","Epoch 66: val_accuracy did not improve from 0.68008\n","\n","Epoch 66: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.8245 - accuracy: 0.7098 - val_loss: 1.0569 - val_accuracy: 0.6674\n","Epoch 67/300\n","350/350 [==============================] - ETA: 0s - loss: 0.8182 - accuracy: 0.7061\n","Epoch 67: val_accuracy did not improve from 0.68008\n","\n","Epoch 67: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.8182 - accuracy: 0.7061 - val_loss: 1.0966 - val_accuracy: 0.6377\n","Epoch 68/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.8467 - accuracy: 0.7073\n","Epoch 68: val_accuracy did not improve from 0.68008\n","\n","Epoch 68: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.8483 - accuracy: 0.7073 - val_loss: 1.0921 - val_accuracy: 0.6547\n","Epoch 69/300\n","350/350 [==============================] - ETA: 0s - loss: 0.8216 - accuracy: 0.7120\n","Epoch 69: val_accuracy did not improve from 0.68008\n","\n","Epoch 69: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.8216 - accuracy: 0.7120 - val_loss: 1.1369 - val_accuracy: 0.6589\n","Epoch 70/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.8367 - accuracy: 0.7156\n","Epoch 70: val_accuracy did not improve from 0.68008\n","\n","Epoch 70: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.8373 - accuracy: 0.7154 - val_loss: 1.1094 - val_accuracy: 0.6589\n","Epoch 71/300\n","350/350 [==============================] - ETA: 0s - loss: 0.8258 - accuracy: 0.7113\n","Epoch 71: val_accuracy did not improve from 0.68008\n","\n","Epoch 71: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.8258 - accuracy: 0.7113 - val_loss: 1.0663 - val_accuracy: 0.6716\n","Epoch 72/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.8396 - accuracy: 0.7071\n","Epoch 72: val_accuracy did not improve from 0.68008\n","\n","Epoch 72: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.8390 - accuracy: 0.7073 - val_loss: 1.1085 - val_accuracy: 0.6653\n","Epoch 73/300\n","350/350 [==============================] - ETA: 0s - loss: 0.8210 - accuracy: 0.7152\n","Epoch 73: val_accuracy did not improve from 0.68008\n","\n","Epoch 73: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.8210 - accuracy: 0.7152 - val_loss: 1.1739 - val_accuracy: 0.6483\n","Epoch 74/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.8138 - accuracy: 0.7219\n","Epoch 74: val_accuracy did not improve from 0.68008\n","\n","Epoch 74: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.8132 - accuracy: 0.7223 - val_loss: 1.1316 - val_accuracy: 0.6419\n","Epoch 75/300\n","350/350 [==============================] - ETA: 0s - loss: 0.8062 - accuracy: 0.7173\n","Epoch 75: val_accuracy did not improve from 0.68008\n","\n","Epoch 75: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.8062 - accuracy: 0.7173 - val_loss: 1.1018 - val_accuracy: 0.6695\n","Epoch 76/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.7990 - accuracy: 0.7204\n","Epoch 76: val_accuracy did not improve from 0.68008\n","\n","Epoch 76: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.8000 - accuracy: 0.7200 - val_loss: 1.0997 - val_accuracy: 0.6483\n","Epoch 77/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.7869 - accuracy: 0.7232\n","Epoch 77: val_accuracy did not improve from 0.68008\n","\n","Epoch 77: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7876 - accuracy: 0.7229 - val_loss: 1.0930 - val_accuracy: 0.6737\n","Epoch 78/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.7811 - accuracy: 0.7292\n","Epoch 78: val_accuracy did not improve from 0.68008\n","\n","Epoch 78: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7822 - accuracy: 0.7291 - val_loss: 1.1064 - val_accuracy: 0.6610\n","Epoch 79/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.7911 - accuracy: 0.7284\n","Epoch 79: val_accuracy did not improve from 0.68008\n","\n","Epoch 79: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7911 - accuracy: 0.7286 - val_loss: 1.0813 - val_accuracy: 0.6483\n","Epoch 80/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.7959 - accuracy: 0.7251\n","Epoch 80: val_accuracy did not improve from 0.68008\n","\n","Epoch 80: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7953 - accuracy: 0.7254 - val_loss: 1.1044 - val_accuracy: 0.6568\n","Epoch 81/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.7758 - accuracy: 0.7282\n","Epoch 81: val_accuracy did not improve from 0.68008\n","\n","Epoch 81: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7747 - accuracy: 0.7284 - val_loss: 1.1315 - val_accuracy: 0.6780\n","Epoch 82/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.7839 - accuracy: 0.7271\n","Epoch 82: val_accuracy did not improve from 0.68008\n","\n","Epoch 82: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7830 - accuracy: 0.7273 - val_loss: 1.1996 - val_accuracy: 0.6314\n","Epoch 83/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.7735 - accuracy: 0.7265\n","Epoch 83: val_accuracy did not improve from 0.68008\n","\n","Epoch 83: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7734 - accuracy: 0.7266 - val_loss: 1.0717 - val_accuracy: 0.6737\n","Epoch 84/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.7827 - accuracy: 0.7362\n","Epoch 84: val_accuracy did not improve from 0.68008\n","\n","Epoch 84: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7814 - accuracy: 0.7366 - val_loss: 1.1824 - val_accuracy: 0.6398\n","Epoch 85/300\n","350/350 [==============================] - ETA: 0s - loss: 0.7994 - accuracy: 0.7209\n","Epoch 85: val_accuracy did not improve from 0.68008\n","\n","Epoch 85: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7994 - accuracy: 0.7209 - val_loss: 1.0504 - val_accuracy: 0.6758\n","Epoch 86/300\n","350/350 [==============================] - ETA: 0s - loss: 0.7858 - accuracy: 0.7125\n","Epoch 86: val_accuracy did not improve from 0.68008\n","\n","Epoch 86: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7858 - accuracy: 0.7125 - val_loss: 1.1596 - val_accuracy: 0.6419\n","Epoch 87/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.7636 - accuracy: 0.7363\n","Epoch 87: val_accuracy did not improve from 0.68008\n","\n","Epoch 87: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7649 - accuracy: 0.7359 - val_loss: 1.0941 - val_accuracy: 0.6674\n","Epoch 88/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.7970 - accuracy: 0.7210\n","Epoch 88: val_accuracy did not improve from 0.68008\n","\n","Epoch 88: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7978 - accuracy: 0.7209 - val_loss: 1.1909 - val_accuracy: 0.6631\n","Epoch 89/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.7590 - accuracy: 0.7360\n","Epoch 89: val_accuracy did not improve from 0.68008\n","\n","Epoch 89: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7583 - accuracy: 0.7364 - val_loss: 1.0749 - val_accuracy: 0.6653\n","Epoch 90/300\n","350/350 [==============================] - ETA: 0s - loss: 0.7563 - accuracy: 0.7375\n","Epoch 90: val_accuracy did not improve from 0.68008\n","\n","Epoch 90: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7563 - accuracy: 0.7375 - val_loss: 1.0706 - val_accuracy: 0.6716\n","Epoch 91/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.7748 - accuracy: 0.7267\n","Epoch 91: val_accuracy did not improve from 0.68008\n","\n","Epoch 91: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 18ms/step - loss: 0.7739 - accuracy: 0.7270 - val_loss: 1.0611 - val_accuracy: 0.6568\n","Epoch 92/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.7540 - accuracy: 0.7362\n","Epoch 92: val_accuracy did not improve from 0.68008\n","\n","Epoch 92: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7549 - accuracy: 0.7359 - val_loss: 1.0671 - val_accuracy: 0.6695\n","Epoch 93/300\n","350/350 [==============================] - ETA: 0s - loss: 0.7579 - accuracy: 0.7380\n","Epoch 93: val_accuracy did not improve from 0.68008\n","\n","Epoch 93: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7579 - accuracy: 0.7380 - val_loss: 1.0624 - val_accuracy: 0.6483\n","Epoch 94/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.7619 - accuracy: 0.7399\n","Epoch 94: val_accuracy did not improve from 0.68008\n","\n","Epoch 94: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7613 - accuracy: 0.7404 - val_loss: 1.0087 - val_accuracy: 0.6674\n","Epoch 95/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.7861 - accuracy: 0.7321\n","Epoch 95: val_accuracy did not improve from 0.68008\n","\n","Epoch 95: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7857 - accuracy: 0.7323 - val_loss: 1.0817 - val_accuracy: 0.6271\n","Epoch 96/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.7523 - accuracy: 0.7387\n","Epoch 96: val_accuracy did not improve from 0.68008\n","\n","Epoch 96: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7513 - accuracy: 0.7391 - val_loss: 1.0229 - val_accuracy: 0.6674\n","Epoch 97/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.7695 - accuracy: 0.7288\n","Epoch 97: val_accuracy did not improve from 0.68008\n","\n","Epoch 97: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7680 - accuracy: 0.7295 - val_loss: 1.0889 - val_accuracy: 0.6758\n","Epoch 98/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.7408 - accuracy: 0.7432\n","Epoch 98: val_accuracy did not improve from 0.68008\n","\n","Epoch 98: val_loss did not improve from 0.95968\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7403 - accuracy: 0.7432 - val_loss: 1.1071 - val_accuracy: 0.6589\n","Epoch 99/300\n","350/350 [==============================] - ETA: 0s - loss: 0.7443 - accuracy: 0.7400\n","Epoch 99: val_accuracy improved from 0.68008 to 0.70551, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 99: val_loss improved from 0.95968 to 0.94320, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 14s 40ms/step - loss: 0.7443 - accuracy: 0.7400 - val_loss: 0.9432 - val_accuracy: 0.7055\n","Epoch 100/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.7393 - accuracy: 0.7477\n","Epoch 100: val_accuracy did not improve from 0.70551\n","\n","Epoch 100: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 18ms/step - loss: 0.7390 - accuracy: 0.7479 - val_loss: 0.9856 - val_accuracy: 0.6907\n","Epoch 101/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.7287 - accuracy: 0.7453\n","Epoch 101: val_accuracy did not improve from 0.70551\n","\n","Epoch 101: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7293 - accuracy: 0.7454 - val_loss: 1.0146 - val_accuracy: 0.6949\n","Epoch 102/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.7554 - accuracy: 0.7446\n","Epoch 102: val_accuracy did not improve from 0.70551\n","\n","Epoch 102: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7562 - accuracy: 0.7446 - val_loss: 1.1519 - val_accuracy: 0.6504\n","Epoch 103/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.7398 - accuracy: 0.7414\n","Epoch 103: val_accuracy did not improve from 0.70551\n","\n","Epoch 103: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7416 - accuracy: 0.7420 - val_loss: 1.1408 - val_accuracy: 0.6695\n","Epoch 104/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.7430 - accuracy: 0.7401\n","Epoch 104: val_accuracy did not improve from 0.70551\n","\n","Epoch 104: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7431 - accuracy: 0.7404 - val_loss: 1.0341 - val_accuracy: 0.6610\n","Epoch 105/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.7396 - accuracy: 0.7409\n","Epoch 105: val_accuracy did not improve from 0.70551\n","\n","Epoch 105: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 18ms/step - loss: 0.7395 - accuracy: 0.7407 - val_loss: 1.0080 - val_accuracy: 0.6568\n","Epoch 106/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.7329 - accuracy: 0.7473\n","Epoch 106: val_accuracy did not improve from 0.70551\n","\n","Epoch 106: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7320 - accuracy: 0.7482 - val_loss: 1.2038 - val_accuracy: 0.6250\n","Epoch 107/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.7238 - accuracy: 0.7496\n","Epoch 107: val_accuracy did not improve from 0.70551\n","\n","Epoch 107: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7243 - accuracy: 0.7496 - val_loss: 1.1110 - val_accuracy: 0.6653\n","Epoch 108/300\n","350/350 [==============================] - ETA: 0s - loss: 0.7089 - accuracy: 0.7548\n","Epoch 108: val_accuracy did not improve from 0.70551\n","\n","Epoch 108: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7089 - accuracy: 0.7548 - val_loss: 1.2752 - val_accuracy: 0.6483\n","Epoch 109/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.7286 - accuracy: 0.7421\n","Epoch 109: val_accuracy did not improve from 0.70551\n","\n","Epoch 109: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 18ms/step - loss: 0.7287 - accuracy: 0.7423 - val_loss: 1.1988 - val_accuracy: 0.6589\n","Epoch 110/300\n","350/350 [==============================] - ETA: 0s - loss: 0.7212 - accuracy: 0.7486\n","Epoch 110: val_accuracy did not improve from 0.70551\n","\n","Epoch 110: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 18ms/step - loss: 0.7212 - accuracy: 0.7486 - val_loss: 0.9759 - val_accuracy: 0.6907\n","Epoch 111/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.7227 - accuracy: 0.7484\n","Epoch 111: val_accuracy did not improve from 0.70551\n","\n","Epoch 111: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7231 - accuracy: 0.7482 - val_loss: 1.0146 - val_accuracy: 0.6674\n","Epoch 112/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.7198 - accuracy: 0.7471\n","Epoch 112: val_accuracy improved from 0.70551 to 0.71398, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 112: val_loss did not improve from 0.94320\n","350/350 [==============================] - 9s 27ms/step - loss: 0.7216 - accuracy: 0.7468 - val_loss: 0.9784 - val_accuracy: 0.7140\n","Epoch 113/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.7121 - accuracy: 0.7543\n","Epoch 113: val_accuracy did not improve from 0.71398\n","\n","Epoch 113: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7139 - accuracy: 0.7539 - val_loss: 0.9830 - val_accuracy: 0.6970\n","Epoch 114/300\n","350/350 [==============================] - ETA: 0s - loss: 0.7169 - accuracy: 0.7473\n","Epoch 114: val_accuracy did not improve from 0.71398\n","\n","Epoch 114: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7169 - accuracy: 0.7473 - val_loss: 1.1655 - val_accuracy: 0.6695\n","Epoch 115/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.7440 - accuracy: 0.7407\n","Epoch 115: val_accuracy did not improve from 0.71398\n","\n","Epoch 115: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7427 - accuracy: 0.7414 - val_loss: 1.0944 - val_accuracy: 0.6864\n","Epoch 116/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.7223 - accuracy: 0.7505\n","Epoch 116: val_accuracy did not improve from 0.71398\n","\n","Epoch 116: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7225 - accuracy: 0.7507 - val_loss: 1.0795 - val_accuracy: 0.6992\n","Epoch 117/300\n","350/350 [==============================] - ETA: 0s - loss: 0.7354 - accuracy: 0.7518\n","Epoch 117: val_accuracy did not improve from 0.71398\n","\n","Epoch 117: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7354 - accuracy: 0.7518 - val_loss: 1.0831 - val_accuracy: 0.6758\n","Epoch 118/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.7352 - accuracy: 0.7434\n","Epoch 118: val_accuracy did not improve from 0.71398\n","\n","Epoch 118: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7347 - accuracy: 0.7434 - val_loss: 0.9623 - val_accuracy: 0.6801\n","Epoch 119/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.7294 - accuracy: 0.7468\n","Epoch 119: val_accuracy did not improve from 0.71398\n","\n","Epoch 119: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7281 - accuracy: 0.7475 - val_loss: 0.9480 - val_accuracy: 0.6907\n","Epoch 120/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.7053 - accuracy: 0.7522\n","Epoch 120: val_accuracy did not improve from 0.71398\n","\n","Epoch 120: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7075 - accuracy: 0.7514 - val_loss: 0.9724 - val_accuracy: 0.6970\n","Epoch 121/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.6803 - accuracy: 0.7655\n","Epoch 121: val_accuracy did not improve from 0.71398\n","\n","Epoch 121: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6802 - accuracy: 0.7657 - val_loss: 1.0431 - val_accuracy: 0.6801\n","Epoch 122/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.7046 - accuracy: 0.7583\n","Epoch 122: val_accuracy did not improve from 0.71398\n","\n","Epoch 122: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7038 - accuracy: 0.7580 - val_loss: 1.0483 - val_accuracy: 0.6801\n","Epoch 123/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.7151 - accuracy: 0.7552\n","Epoch 123: val_accuracy did not improve from 0.71398\n","\n","Epoch 123: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7149 - accuracy: 0.7552 - val_loss: 1.0248 - val_accuracy: 0.7034\n","Epoch 124/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6796 - accuracy: 0.7651\n","Epoch 124: val_accuracy did not improve from 0.71398\n","\n","Epoch 124: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6812 - accuracy: 0.7646 - val_loss: 1.1549 - val_accuracy: 0.6864\n","Epoch 125/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.7169 - accuracy: 0.7434\n","Epoch 125: val_accuracy did not improve from 0.71398\n","\n","Epoch 125: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7164 - accuracy: 0.7437 - val_loss: 0.9794 - val_accuracy: 0.7013\n","Epoch 126/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.7132 - accuracy: 0.7530\n","Epoch 126: val_accuracy did not improve from 0.71398\n","\n","Epoch 126: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7133 - accuracy: 0.7529 - val_loss: 1.0046 - val_accuracy: 0.6992\n","Epoch 127/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6836 - accuracy: 0.7609\n","Epoch 127: val_accuracy did not improve from 0.71398\n","\n","Epoch 127: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6827 - accuracy: 0.7613 - val_loss: 1.2024 - val_accuracy: 0.6589\n","Epoch 128/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6734 - accuracy: 0.7698\n","Epoch 128: val_accuracy did not improve from 0.71398\n","\n","Epoch 128: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6747 - accuracy: 0.7696 - val_loss: 0.9895 - val_accuracy: 0.6716\n","Epoch 129/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6862 - accuracy: 0.7596\n","Epoch 129: val_accuracy did not improve from 0.71398\n","\n","Epoch 129: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6862 - accuracy: 0.7596 - val_loss: 1.1043 - val_accuracy: 0.6737\n","Epoch 130/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.7058 - accuracy: 0.7572\n","Epoch 130: val_accuracy did not improve from 0.71398\n","\n","Epoch 130: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7044 - accuracy: 0.7575 - val_loss: 1.3367 - val_accuracy: 0.6377\n","Epoch 131/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.7097 - accuracy: 0.7559\n","Epoch 131: val_accuracy did not improve from 0.71398\n","\n","Epoch 131: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7103 - accuracy: 0.7557 - val_loss: 0.9608 - val_accuracy: 0.6758\n","Epoch 132/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.7609\n","Epoch 132: val_accuracy did not improve from 0.71398\n","\n","Epoch 132: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6931 - accuracy: 0.7609 - val_loss: 1.0908 - val_accuracy: 0.6589\n","Epoch 133/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6901 - accuracy: 0.7534\n","Epoch 133: val_accuracy did not improve from 0.71398\n","\n","Epoch 133: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6915 - accuracy: 0.7530 - val_loss: 1.1262 - val_accuracy: 0.6589\n","Epoch 134/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6960 - accuracy: 0.7652\n","Epoch 134: val_accuracy did not improve from 0.71398\n","\n","Epoch 134: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6960 - accuracy: 0.7654 - val_loss: 1.0189 - val_accuracy: 0.6864\n","Epoch 135/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6727 - accuracy: 0.7726\n","Epoch 135: val_accuracy did not improve from 0.71398\n","\n","Epoch 135: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6727 - accuracy: 0.7723 - val_loss: 1.1028 - val_accuracy: 0.6780\n","Epoch 136/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6755 - accuracy: 0.7673\n","Epoch 136: val_accuracy did not improve from 0.71398\n","\n","Epoch 136: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6755 - accuracy: 0.7673 - val_loss: 1.1378 - val_accuracy: 0.6780\n","Epoch 137/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6854 - accuracy: 0.7651\n","Epoch 137: val_accuracy did not improve from 0.71398\n","\n","Epoch 137: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6855 - accuracy: 0.7655 - val_loss: 1.0070 - val_accuracy: 0.6674\n","Epoch 138/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6765 - accuracy: 0.7633\n","Epoch 138: val_accuracy did not improve from 0.71398\n","\n","Epoch 138: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6755 - accuracy: 0.7636 - val_loss: 1.0228 - val_accuracy: 0.6886\n","Epoch 139/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.7649\n","Epoch 139: val_accuracy did not improve from 0.71398\n","\n","Epoch 139: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6921 - accuracy: 0.7643 - val_loss: 1.1631 - val_accuracy: 0.6949\n","Epoch 140/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6902 - accuracy: 0.7636\n","Epoch 140: val_accuracy did not improve from 0.71398\n","\n","Epoch 140: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6898 - accuracy: 0.7636 - val_loss: 0.9935 - val_accuracy: 0.7097\n","Epoch 141/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6959 - accuracy: 0.7572\n","Epoch 141: val_accuracy did not improve from 0.71398\n","\n","Epoch 141: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6956 - accuracy: 0.7573 - val_loss: 1.0967 - val_accuracy: 0.6695\n","Epoch 142/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6405 - accuracy: 0.7814\n","Epoch 142: val_accuracy did not improve from 0.71398\n","\n","Epoch 142: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6416 - accuracy: 0.7811 - val_loss: 1.1481 - val_accuracy: 0.6907\n","Epoch 143/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.7031 - accuracy: 0.7588\n","Epoch 143: val_accuracy did not improve from 0.71398\n","\n","Epoch 143: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.7021 - accuracy: 0.7593 - val_loss: 1.2114 - val_accuracy: 0.6695\n","Epoch 144/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6661 - accuracy: 0.7736\n","Epoch 144: val_accuracy did not improve from 0.71398\n","\n","Epoch 144: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6661 - accuracy: 0.7736 - val_loss: 1.0606 - val_accuracy: 0.6907\n","Epoch 145/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6704 - accuracy: 0.7710\n","Epoch 145: val_accuracy did not improve from 0.71398\n","\n","Epoch 145: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6701 - accuracy: 0.7711 - val_loss: 1.2348 - val_accuracy: 0.6356\n","Epoch 146/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6617 - accuracy: 0.7638\n","Epoch 146: val_accuracy did not improve from 0.71398\n","\n","Epoch 146: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6617 - accuracy: 0.7638 - val_loss: 1.1305 - val_accuracy: 0.6525\n","Epoch 147/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.6553 - accuracy: 0.7725\n","Epoch 147: val_accuracy did not improve from 0.71398\n","\n","Epoch 147: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6549 - accuracy: 0.7725 - val_loss: 1.0900 - val_accuracy: 0.6568\n","Epoch 148/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6725 - accuracy: 0.7649\n","Epoch 148: val_accuracy did not improve from 0.71398\n","\n","Epoch 148: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6716 - accuracy: 0.7652 - val_loss: 1.0904 - val_accuracy: 0.6716\n","Epoch 149/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6700 - accuracy: 0.7654\n","Epoch 149: val_accuracy did not improve from 0.71398\n","\n","Epoch 149: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6711 - accuracy: 0.7652 - val_loss: 1.0228 - val_accuracy: 0.6864\n","Epoch 150/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6590 - accuracy: 0.7711\n","Epoch 150: val_accuracy did not improve from 0.71398\n","\n","Epoch 150: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6590 - accuracy: 0.7711 - val_loss: 1.0019 - val_accuracy: 0.6907\n","Epoch 151/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6728 - accuracy: 0.7698\n","Epoch 151: val_accuracy did not improve from 0.71398\n","\n","Epoch 151: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6720 - accuracy: 0.7705 - val_loss: 1.1805 - val_accuracy: 0.6949\n","Epoch 152/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6585 - accuracy: 0.7784\n","Epoch 152: val_accuracy did not improve from 0.71398\n","\n","Epoch 152: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6601 - accuracy: 0.7779 - val_loss: 1.0031 - val_accuracy: 0.6928\n","Epoch 153/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6628 - accuracy: 0.7719\n","Epoch 153: val_accuracy did not improve from 0.71398\n","\n","Epoch 153: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6625 - accuracy: 0.7720 - val_loss: 1.1053 - val_accuracy: 0.6547\n","Epoch 154/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6707 - accuracy: 0.7621\n","Epoch 154: val_accuracy did not improve from 0.71398\n","\n","Epoch 154: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6707 - accuracy: 0.7621 - val_loss: 1.1115 - val_accuracy: 0.6843\n","Epoch 155/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6575 - accuracy: 0.7727\n","Epoch 155: val_accuracy did not improve from 0.71398\n","\n","Epoch 155: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6571 - accuracy: 0.7729 - val_loss: 1.0753 - val_accuracy: 0.6547\n","Epoch 156/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6958 - accuracy: 0.7608\n","Epoch 156: val_accuracy did not improve from 0.71398\n","\n","Epoch 156: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6948 - accuracy: 0.7609 - val_loss: 1.1900 - val_accuracy: 0.6907\n","Epoch 157/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.6440 - accuracy: 0.7723\n","Epoch 157: val_accuracy improved from 0.71398 to 0.72034, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 157: val_loss did not improve from 0.94320\n","350/350 [==============================] - 9s 27ms/step - loss: 0.6430 - accuracy: 0.7730 - val_loss: 1.0627 - val_accuracy: 0.7203\n","Epoch 158/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6598 - accuracy: 0.7731\n","Epoch 158: val_accuracy did not improve from 0.72034\n","\n","Epoch 158: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6591 - accuracy: 0.7734 - val_loss: 0.9887 - val_accuracy: 0.7055\n","Epoch 159/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6767 - accuracy: 0.7677\n","Epoch 159: val_accuracy did not improve from 0.72034\n","\n","Epoch 159: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6767 - accuracy: 0.7677 - val_loss: 1.0443 - val_accuracy: 0.7055\n","Epoch 160/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.6839 - accuracy: 0.7695\n","Epoch 160: val_accuracy did not improve from 0.72034\n","\n","Epoch 160: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6829 - accuracy: 0.7696 - val_loss: 1.0655 - val_accuracy: 0.6758\n","Epoch 161/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6696 - accuracy: 0.7663\n","Epoch 161: val_accuracy did not improve from 0.72034\n","\n","Epoch 161: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6694 - accuracy: 0.7664 - val_loss: 1.1604 - val_accuracy: 0.6716\n","Epoch 162/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6627 - accuracy: 0.7795\n","Epoch 162: val_accuracy did not improve from 0.72034\n","\n","Epoch 162: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6625 - accuracy: 0.7798 - val_loss: 1.0421 - val_accuracy: 0.7097\n","Epoch 163/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6623 - accuracy: 0.7728\n","Epoch 163: val_accuracy did not improve from 0.72034\n","\n","Epoch 163: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6620 - accuracy: 0.7729 - val_loss: 1.0748 - val_accuracy: 0.6907\n","Epoch 164/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6680 - accuracy: 0.7745\n","Epoch 164: val_accuracy did not improve from 0.72034\n","\n","Epoch 164: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6680 - accuracy: 0.7745 - val_loss: 1.1059 - val_accuracy: 0.6653\n","Epoch 165/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6545 - accuracy: 0.7750\n","Epoch 165: val_accuracy did not improve from 0.72034\n","\n","Epoch 165: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6545 - accuracy: 0.7750 - val_loss: 1.1124 - val_accuracy: 0.6992\n","Epoch 166/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6583 - accuracy: 0.7721\n","Epoch 166: val_accuracy did not improve from 0.72034\n","\n","Epoch 166: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6583 - accuracy: 0.7721 - val_loss: 1.0695 - val_accuracy: 0.6737\n","Epoch 167/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6446 - accuracy: 0.7817\n","Epoch 167: val_accuracy did not improve from 0.72034\n","\n","Epoch 167: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6446 - accuracy: 0.7820 - val_loss: 1.1081 - val_accuracy: 0.6907\n","Epoch 168/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6759 - accuracy: 0.7709\n","Epoch 168: val_accuracy did not improve from 0.72034\n","\n","Epoch 168: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6759 - accuracy: 0.7709 - val_loss: 0.9769 - val_accuracy: 0.7076\n","Epoch 169/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6492 - accuracy: 0.7727\n","Epoch 169: val_accuracy improved from 0.72034 to 0.72458, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 169: val_loss did not improve from 0.94320\n","350/350 [==============================] - 9s 26ms/step - loss: 0.6492 - accuracy: 0.7727 - val_loss: 1.0655 - val_accuracy: 0.7246\n","Epoch 170/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6490 - accuracy: 0.7730\n","Epoch 170: val_accuracy did not improve from 0.72458\n","\n","Epoch 170: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6490 - accuracy: 0.7730 - val_loss: 1.1743 - val_accuracy: 0.6970\n","Epoch 171/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6557 - accuracy: 0.7770\n","Epoch 171: val_accuracy did not improve from 0.72458\n","\n","Epoch 171: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6557 - accuracy: 0.7770 - val_loss: 1.0949 - val_accuracy: 0.7119\n","Epoch 172/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6516 - accuracy: 0.7724\n","Epoch 172: val_accuracy did not improve from 0.72458\n","\n","Epoch 172: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6518 - accuracy: 0.7723 - val_loss: 1.0972 - val_accuracy: 0.7034\n","Epoch 173/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.6504 - accuracy: 0.7758\n","Epoch 173: val_accuracy did not improve from 0.72458\n","\n","Epoch 173: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6521 - accuracy: 0.7754 - val_loss: 1.0423 - val_accuracy: 0.7013\n","Epoch 174/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6530 - accuracy: 0.7779\n","Epoch 174: val_accuracy did not improve from 0.72458\n","\n","Epoch 174: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6519 - accuracy: 0.7786 - val_loss: 0.9718 - val_accuracy: 0.7034\n","Epoch 175/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6256 - accuracy: 0.7845\n","Epoch 175: val_accuracy did not improve from 0.72458\n","\n","Epoch 175: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6256 - accuracy: 0.7845 - val_loss: 0.9523 - val_accuracy: 0.7182\n","Epoch 176/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6663 - accuracy: 0.7708\n","Epoch 176: val_accuracy did not improve from 0.72458\n","\n","Epoch 176: val_loss did not improve from 0.94320\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6681 - accuracy: 0.7704 - val_loss: 1.0124 - val_accuracy: 0.6843\n","Epoch 177/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6301 - accuracy: 0.7815\n","Epoch 177: val_accuracy did not improve from 0.72458\n","\n","Epoch 177: val_loss improved from 0.94320 to 0.93763, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 9s 27ms/step - loss: 0.6298 - accuracy: 0.7818 - val_loss: 0.9376 - val_accuracy: 0.7013\n","Epoch 178/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6412 - accuracy: 0.7809\n","Epoch 178: val_accuracy did not improve from 0.72458\n","\n","Epoch 178: val_loss improved from 0.93763 to 0.92338, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 9s 27ms/step - loss: 0.6412 - accuracy: 0.7809 - val_loss: 0.9234 - val_accuracy: 0.7034\n","Epoch 179/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6324 - accuracy: 0.7750\n","Epoch 179: val_accuracy did not improve from 0.72458\n","\n","Epoch 179: val_loss did not improve from 0.92338\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6324 - accuracy: 0.7750 - val_loss: 1.0934 - val_accuracy: 0.7055\n","Epoch 180/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6402 - accuracy: 0.7812\n","Epoch 180: val_accuracy did not improve from 0.72458\n","\n","Epoch 180: val_loss did not improve from 0.92338\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6396 - accuracy: 0.7812 - val_loss: 0.9411 - val_accuracy: 0.7013\n","Epoch 181/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6390 - accuracy: 0.7801\n","Epoch 181: val_accuracy did not improve from 0.72458\n","\n","Epoch 181: val_loss did not improve from 0.92338\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6380 - accuracy: 0.7805 - val_loss: 0.9755 - val_accuracy: 0.6970\n","Epoch 182/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6447 - accuracy: 0.7838\n","Epoch 182: val_accuracy did not improve from 0.72458\n","\n","Epoch 182: val_loss did not improve from 0.92338\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6454 - accuracy: 0.7834 - val_loss: 0.9620 - val_accuracy: 0.7076\n","Epoch 183/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6235 - accuracy: 0.7812\n","Epoch 183: val_accuracy did not improve from 0.72458\n","\n","Epoch 183: val_loss improved from 0.92338 to 0.86079, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","350/350 [==============================] - 9s 27ms/step - loss: 0.6235 - accuracy: 0.7812 - val_loss: 0.8608 - val_accuracy: 0.7225\n","Epoch 184/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6618 - accuracy: 0.7763\n","Epoch 184: val_accuracy did not improve from 0.72458\n","\n","Epoch 184: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6613 - accuracy: 0.7766 - val_loss: 0.9610 - val_accuracy: 0.7055\n","Epoch 185/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6519 - accuracy: 0.7745\n","Epoch 185: val_accuracy improved from 0.72458 to 0.74153, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 185: val_loss did not improve from 0.86079\n","350/350 [==============================] - 9s 27ms/step - loss: 0.6510 - accuracy: 0.7750 - val_loss: 0.8716 - val_accuracy: 0.7415\n","Epoch 186/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6140 - accuracy: 0.7906\n","Epoch 186: val_accuracy did not improve from 0.74153\n","\n","Epoch 186: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6145 - accuracy: 0.7907 - val_loss: 1.0044 - val_accuracy: 0.6992\n","Epoch 187/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6614 - accuracy: 0.7708\n","Epoch 187: val_accuracy did not improve from 0.74153\n","\n","Epoch 187: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6606 - accuracy: 0.7707 - val_loss: 1.0017 - val_accuracy: 0.7097\n","Epoch 188/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6153 - accuracy: 0.7889\n","Epoch 188: val_accuracy did not improve from 0.74153\n","\n","Epoch 188: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6153 - accuracy: 0.7889 - val_loss: 1.1148 - val_accuracy: 0.6716\n","Epoch 189/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6852 - accuracy: 0.7735\n","Epoch 189: val_accuracy did not improve from 0.74153\n","\n","Epoch 189: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6847 - accuracy: 0.7739 - val_loss: 0.9158 - val_accuracy: 0.7161\n","Epoch 190/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6368 - accuracy: 0.7795\n","Epoch 190: val_accuracy did not improve from 0.74153\n","\n","Epoch 190: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6368 - accuracy: 0.7795 - val_loss: 0.9587 - val_accuracy: 0.7055\n","Epoch 191/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6459 - accuracy: 0.7845\n","Epoch 191: val_accuracy did not improve from 0.74153\n","\n","Epoch 191: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6463 - accuracy: 0.7843 - val_loss: 1.0225 - val_accuracy: 0.7119\n","Epoch 192/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6565 - accuracy: 0.7775\n","Epoch 192: val_accuracy did not improve from 0.74153\n","\n","Epoch 192: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6579 - accuracy: 0.7770 - val_loss: 1.1230 - val_accuracy: 0.6695\n","Epoch 193/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6369 - accuracy: 0.7797\n","Epoch 193: val_accuracy did not improve from 0.74153\n","\n","Epoch 193: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6362 - accuracy: 0.7802 - val_loss: 1.0174 - val_accuracy: 0.6907\n","Epoch 194/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6446 - accuracy: 0.7771\n","Epoch 194: val_accuracy did not improve from 0.74153\n","\n","Epoch 194: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6449 - accuracy: 0.7771 - val_loss: 0.9845 - val_accuracy: 0.7119\n","Epoch 195/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6348 - accuracy: 0.7841\n","Epoch 195: val_accuracy did not improve from 0.74153\n","\n","Epoch 195: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6348 - accuracy: 0.7841 - val_loss: 1.0163 - val_accuracy: 0.6928\n","Epoch 196/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6212 - accuracy: 0.7937\n","Epoch 196: val_accuracy did not improve from 0.74153\n","\n","Epoch 196: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6216 - accuracy: 0.7936 - val_loss: 1.1765 - val_accuracy: 0.6674\n","Epoch 197/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6293 - accuracy: 0.7825\n","Epoch 197: val_accuracy did not improve from 0.74153\n","\n","Epoch 197: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6293 - accuracy: 0.7825 - val_loss: 1.1049 - val_accuracy: 0.6610\n","Epoch 198/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.6459 - accuracy: 0.7767\n","Epoch 198: val_accuracy did not improve from 0.74153\n","\n","Epoch 198: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6463 - accuracy: 0.7766 - val_loss: 0.9839 - val_accuracy: 0.7203\n","Epoch 199/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6218 - accuracy: 0.7834\n","Epoch 199: val_accuracy did not improve from 0.74153\n","\n","Epoch 199: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6233 - accuracy: 0.7832 - val_loss: 1.0206 - val_accuracy: 0.6970\n","Epoch 200/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6143 - accuracy: 0.7929\n","Epoch 200: val_accuracy did not improve from 0.74153\n","\n","Epoch 200: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6143 - accuracy: 0.7929 - val_loss: 1.1965 - val_accuracy: 0.6822\n","Epoch 201/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.6314 - accuracy: 0.7835\n","Epoch 201: val_accuracy did not improve from 0.74153\n","\n","Epoch 201: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6305 - accuracy: 0.7841 - val_loss: 0.8915 - val_accuracy: 0.7182\n","Epoch 202/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6196 - accuracy: 0.7896\n","Epoch 202: val_accuracy did not improve from 0.74153\n","\n","Epoch 202: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6205 - accuracy: 0.7898 - val_loss: 1.0914 - val_accuracy: 0.6907\n","Epoch 203/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6429 - accuracy: 0.7787\n","Epoch 203: val_accuracy did not improve from 0.74153\n","\n","Epoch 203: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6431 - accuracy: 0.7784 - val_loss: 0.9349 - val_accuracy: 0.7203\n","Epoch 204/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6154 - accuracy: 0.7895\n","Epoch 204: val_accuracy did not improve from 0.74153\n","\n","Epoch 204: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6155 - accuracy: 0.7896 - val_loss: 0.9873 - val_accuracy: 0.6822\n","Epoch 205/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6357 - accuracy: 0.7855\n","Epoch 205: val_accuracy did not improve from 0.74153\n","\n","Epoch 205: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6353 - accuracy: 0.7854 - val_loss: 0.9539 - val_accuracy: 0.7288\n","Epoch 206/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6152 - accuracy: 0.7872\n","Epoch 206: val_accuracy did not improve from 0.74153\n","\n","Epoch 206: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6169 - accuracy: 0.7871 - val_loss: 0.9405 - val_accuracy: 0.7309\n","Epoch 207/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6333 - accuracy: 0.7809\n","Epoch 207: val_accuracy did not improve from 0.74153\n","\n","Epoch 207: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6333 - accuracy: 0.7809 - val_loss: 0.9896 - val_accuracy: 0.6886\n","Epoch 208/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6418 - accuracy: 0.7814\n","Epoch 208: val_accuracy did not improve from 0.74153\n","\n","Epoch 208: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6419 - accuracy: 0.7818 - val_loss: 0.9851 - val_accuracy: 0.7097\n","Epoch 209/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6039 - accuracy: 0.7930\n","Epoch 209: val_accuracy did not improve from 0.74153\n","\n","Epoch 209: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6034 - accuracy: 0.7930 - val_loss: 1.0005 - val_accuracy: 0.7097\n","Epoch 210/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.6080 - accuracy: 0.7918\n","Epoch 210: val_accuracy did not improve from 0.74153\n","\n","Epoch 210: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6084 - accuracy: 0.7916 - val_loss: 0.9598 - val_accuracy: 0.7140\n","Epoch 211/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6120 - accuracy: 0.7873\n","Epoch 211: val_accuracy did not improve from 0.74153\n","\n","Epoch 211: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6120 - accuracy: 0.7873 - val_loss: 0.9680 - val_accuracy: 0.6949\n","Epoch 212/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.6298 - accuracy: 0.7824\n","Epoch 212: val_accuracy did not improve from 0.74153\n","\n","Epoch 212: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6298 - accuracy: 0.7825 - val_loss: 1.0436 - val_accuracy: 0.6674\n","Epoch 213/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6332 - accuracy: 0.7844\n","Epoch 213: val_accuracy did not improve from 0.74153\n","\n","Epoch 213: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6327 - accuracy: 0.7845 - val_loss: 0.9915 - val_accuracy: 0.7013\n","Epoch 214/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6172 - accuracy: 0.7929\n","Epoch 214: val_accuracy did not improve from 0.74153\n","\n","Epoch 214: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6172 - accuracy: 0.7929 - val_loss: 1.0311 - val_accuracy: 0.7161\n","Epoch 215/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6384 - accuracy: 0.7859\n","Epoch 215: val_accuracy did not improve from 0.74153\n","\n","Epoch 215: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6384 - accuracy: 0.7859 - val_loss: 1.0473 - val_accuracy: 0.7076\n","Epoch 216/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6213 - accuracy: 0.7886\n","Epoch 216: val_accuracy did not improve from 0.74153\n","\n","Epoch 216: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6213 - accuracy: 0.7886 - val_loss: 0.9818 - val_accuracy: 0.7246\n","Epoch 217/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6010 - accuracy: 0.7937\n","Epoch 217: val_accuracy did not improve from 0.74153\n","\n","Epoch 217: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6010 - accuracy: 0.7937 - val_loss: 0.9016 - val_accuracy: 0.7309\n","Epoch 218/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.5975 - accuracy: 0.7989\n","Epoch 218: val_accuracy did not improve from 0.74153\n","\n","Epoch 218: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5966 - accuracy: 0.7989 - val_loss: 1.0999 - val_accuracy: 0.6907\n","Epoch 219/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6012 - accuracy: 0.7993\n","Epoch 219: val_accuracy did not improve from 0.74153\n","\n","Epoch 219: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6012 - accuracy: 0.7993 - val_loss: 1.1321 - val_accuracy: 0.6864\n","Epoch 220/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6356 - accuracy: 0.7851\n","Epoch 220: val_accuracy improved from 0.74153 to 0.75000, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","Epoch 220: val_loss did not improve from 0.86079\n","350/350 [==============================] - 9s 27ms/step - loss: 0.6348 - accuracy: 0.7854 - val_loss: 0.9309 - val_accuracy: 0.7500\n","Epoch 221/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6028 - accuracy: 0.7923\n","Epoch 221: val_accuracy did not improve from 0.75000\n","\n","Epoch 221: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6028 - accuracy: 0.7923 - val_loss: 1.0585 - val_accuracy: 0.6758\n","Epoch 222/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6152 - accuracy: 0.7891\n","Epoch 222: val_accuracy did not improve from 0.75000\n","\n","Epoch 222: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6152 - accuracy: 0.7891 - val_loss: 1.1044 - val_accuracy: 0.6949\n","Epoch 223/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6067 - accuracy: 0.7935\n","Epoch 223: val_accuracy did not improve from 0.75000\n","\n","Epoch 223: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6052 - accuracy: 0.7943 - val_loss: 0.9906 - val_accuracy: 0.7309\n","Epoch 224/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6335 - accuracy: 0.7799\n","Epoch 224: val_accuracy did not improve from 0.75000\n","\n","Epoch 224: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6329 - accuracy: 0.7802 - val_loss: 1.0670 - val_accuracy: 0.6907\n","Epoch 225/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6191 - accuracy: 0.7892\n","Epoch 225: val_accuracy did not improve from 0.75000\n","\n","Epoch 225: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6191 - accuracy: 0.7893 - val_loss: 1.0655 - val_accuracy: 0.6907\n","Epoch 226/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6773 - accuracy: 0.7761\n","Epoch 226: val_accuracy did not improve from 0.75000\n","\n","Epoch 226: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6773 - accuracy: 0.7761 - val_loss: 1.1078 - val_accuracy: 0.6886\n","Epoch 227/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6175 - accuracy: 0.7904\n","Epoch 227: val_accuracy did not improve from 0.75000\n","\n","Epoch 227: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6175 - accuracy: 0.7904 - val_loss: 0.9213 - val_accuracy: 0.7161\n","Epoch 228/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6195 - accuracy: 0.7915\n","Epoch 228: val_accuracy did not improve from 0.75000\n","\n","Epoch 228: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6192 - accuracy: 0.7916 - val_loss: 1.1919 - val_accuracy: 0.7161\n","Epoch 229/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6346 - accuracy: 0.7839\n","Epoch 229: val_accuracy did not improve from 0.75000\n","\n","Epoch 229: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6346 - accuracy: 0.7839 - val_loss: 1.1563 - val_accuracy: 0.6949\n","Epoch 230/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6366 - accuracy: 0.7893\n","Epoch 230: val_accuracy did not improve from 0.75000\n","\n","Epoch 230: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6366 - accuracy: 0.7893 - val_loss: 1.0206 - val_accuracy: 0.6886\n","Epoch 231/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.6155 - accuracy: 0.7956\n","Epoch 231: val_accuracy did not improve from 0.75000\n","\n","Epoch 231: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6165 - accuracy: 0.7952 - val_loss: 0.9252 - val_accuracy: 0.7246\n","Epoch 232/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6153 - accuracy: 0.7933\n","Epoch 232: val_accuracy did not improve from 0.75000\n","\n","Epoch 232: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6179 - accuracy: 0.7923 - val_loss: 1.0949 - val_accuracy: 0.6822\n","Epoch 233/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.6152 - accuracy: 0.7896\n","Epoch 233: val_accuracy did not improve from 0.75000\n","\n","Epoch 233: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6193 - accuracy: 0.7886 - val_loss: 1.0466 - val_accuracy: 0.6801\n","Epoch 234/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6133 - accuracy: 0.7888\n","Epoch 234: val_accuracy did not improve from 0.75000\n","\n","Epoch 234: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6124 - accuracy: 0.7889 - val_loss: 1.1573 - val_accuracy: 0.6928\n","Epoch 235/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6269 - accuracy: 0.7881\n","Epoch 235: val_accuracy did not improve from 0.75000\n","\n","Epoch 235: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6282 - accuracy: 0.7879 - val_loss: 0.9437 - val_accuracy: 0.7034\n","Epoch 236/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.6238 - accuracy: 0.7905\n","Epoch 236: val_accuracy did not improve from 0.75000\n","\n","Epoch 236: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6265 - accuracy: 0.7895 - val_loss: 1.0415 - val_accuracy: 0.6886\n","Epoch 237/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6075 - accuracy: 0.7952\n","Epoch 237: val_accuracy did not improve from 0.75000\n","\n","Epoch 237: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6075 - accuracy: 0.7952 - val_loss: 1.1276 - val_accuracy: 0.6780\n","Epoch 238/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6253 - accuracy: 0.7857\n","Epoch 238: val_accuracy did not improve from 0.75000\n","\n","Epoch 238: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6244 - accuracy: 0.7862 - val_loss: 1.0488 - val_accuracy: 0.6949\n","Epoch 239/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6173 - accuracy: 0.7884\n","Epoch 239: val_accuracy did not improve from 0.75000\n","\n","Epoch 239: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6171 - accuracy: 0.7886 - val_loss: 1.0412 - val_accuracy: 0.7097\n","Epoch 240/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.5932 - accuracy: 0.7958\n","Epoch 240: val_accuracy did not improve from 0.75000\n","\n","Epoch 240: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5923 - accuracy: 0.7961 - val_loss: 1.0385 - val_accuracy: 0.7394\n","Epoch 241/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.5904 - accuracy: 0.7983\n","Epoch 241: val_accuracy did not improve from 0.75000\n","\n","Epoch 241: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5910 - accuracy: 0.7979 - val_loss: 1.0102 - val_accuracy: 0.7119\n","Epoch 242/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6051 - accuracy: 0.8011\n","Epoch 242: val_accuracy did not improve from 0.75000\n","\n","Epoch 242: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6051 - accuracy: 0.8011 - val_loss: 1.0290 - val_accuracy: 0.7373\n","Epoch 243/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6206 - accuracy: 0.7936\n","Epoch 243: val_accuracy did not improve from 0.75000\n","\n","Epoch 243: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6201 - accuracy: 0.7934 - val_loss: 1.0137 - val_accuracy: 0.7097\n","Epoch 244/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6199 - accuracy: 0.7902\n","Epoch 244: val_accuracy did not improve from 0.75000\n","\n","Epoch 244: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6199 - accuracy: 0.7902 - val_loss: 0.9537 - val_accuracy: 0.7076\n","Epoch 245/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6165 - accuracy: 0.7870\n","Epoch 245: val_accuracy did not improve from 0.75000\n","\n","Epoch 245: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6159 - accuracy: 0.7871 - val_loss: 0.9585 - val_accuracy: 0.7394\n","Epoch 246/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6154 - accuracy: 0.7898\n","Epoch 246: val_accuracy did not improve from 0.75000\n","\n","Epoch 246: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6154 - accuracy: 0.7898 - val_loss: 1.1898 - val_accuracy: 0.6992\n","Epoch 247/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6069 - accuracy: 0.7948\n","Epoch 247: val_accuracy did not improve from 0.75000\n","\n","Epoch 247: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6071 - accuracy: 0.7950 - val_loss: 0.9729 - val_accuracy: 0.7225\n","Epoch 248/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6117 - accuracy: 0.7908\n","Epoch 248: val_accuracy did not improve from 0.75000\n","\n","Epoch 248: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6107 - accuracy: 0.7911 - val_loss: 0.9333 - val_accuracy: 0.7415\n","Epoch 249/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6264 - accuracy: 0.7912\n","Epoch 249: val_accuracy did not improve from 0.75000\n","\n","Epoch 249: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6258 - accuracy: 0.7912 - val_loss: 1.0472 - val_accuracy: 0.7055\n","Epoch 250/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6319 - accuracy: 0.7900\n","Epoch 250: val_accuracy did not improve from 0.75000\n","\n","Epoch 250: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6319 - accuracy: 0.7900 - val_loss: 1.0841 - val_accuracy: 0.6886\n","Epoch 251/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6044 - accuracy: 0.7941\n","Epoch 251: val_accuracy did not improve from 0.75000\n","\n","Epoch 251: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6044 - accuracy: 0.7941 - val_loss: 1.0057 - val_accuracy: 0.7309\n","Epoch 252/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6179 - accuracy: 0.7889\n","Epoch 252: val_accuracy did not improve from 0.75000\n","\n","Epoch 252: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6179 - accuracy: 0.7889 - val_loss: 1.1435 - val_accuracy: 0.6801\n","Epoch 253/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6367 - accuracy: 0.7878\n","Epoch 253: val_accuracy did not improve from 0.75000\n","\n","Epoch 253: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6362 - accuracy: 0.7877 - val_loss: 1.2434 - val_accuracy: 0.6801\n","Epoch 254/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6169 - accuracy: 0.7979\n","Epoch 254: val_accuracy did not improve from 0.75000\n","\n","Epoch 254: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6169 - accuracy: 0.7979 - val_loss: 1.0479 - val_accuracy: 0.6843\n","Epoch 255/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6017 - accuracy: 0.7972\n","Epoch 255: val_accuracy did not improve from 0.75000\n","\n","Epoch 255: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6026 - accuracy: 0.7970 - val_loss: 1.0247 - val_accuracy: 0.6801\n","Epoch 256/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.5894 - accuracy: 0.8023\n","Epoch 256: val_accuracy did not improve from 0.75000\n","\n","Epoch 256: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5885 - accuracy: 0.8025 - val_loss: 1.1565 - val_accuracy: 0.6780\n","Epoch 257/300\n","350/350 [==============================] - ETA: 0s - loss: 0.5787 - accuracy: 0.8061\n","Epoch 257: val_accuracy did not improve from 0.75000\n","\n","Epoch 257: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.5787 - accuracy: 0.8061 - val_loss: 1.0979 - val_accuracy: 0.6886\n","Epoch 258/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6095 - accuracy: 0.7948\n","Epoch 258: val_accuracy did not improve from 0.75000\n","\n","Epoch 258: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6094 - accuracy: 0.7948 - val_loss: 1.0001 - val_accuracy: 0.7076\n","Epoch 259/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6006 - accuracy: 0.7975\n","Epoch 259: val_accuracy did not improve from 0.75000\n","\n","Epoch 259: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6006 - accuracy: 0.7975 - val_loss: 0.9501 - val_accuracy: 0.7246\n","Epoch 260/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6023 - accuracy: 0.7957\n","Epoch 260: val_accuracy did not improve from 0.75000\n","\n","Epoch 260: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6019 - accuracy: 0.7961 - val_loss: 1.0883 - val_accuracy: 0.6949\n","Epoch 261/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6055 - accuracy: 0.8014\n","Epoch 261: val_accuracy did not improve from 0.75000\n","\n","Epoch 261: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6054 - accuracy: 0.8012 - val_loss: 1.0161 - val_accuracy: 0.7119\n","Epoch 262/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6228 - accuracy: 0.7926\n","Epoch 262: val_accuracy did not improve from 0.75000\n","\n","Epoch 262: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6222 - accuracy: 0.7925 - val_loss: 0.9926 - val_accuracy: 0.7076\n","Epoch 263/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6146 - accuracy: 0.7895\n","Epoch 263: val_accuracy did not improve from 0.75000\n","\n","Epoch 263: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6160 - accuracy: 0.7893 - val_loss: 1.0588 - val_accuracy: 0.7161\n","Epoch 264/300\n","350/350 [==============================] - ETA: 0s - loss: 0.5980 - accuracy: 0.8030\n","Epoch 264: val_accuracy did not improve from 0.75000\n","\n","Epoch 264: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5980 - accuracy: 0.8030 - val_loss: 0.9621 - val_accuracy: 0.6949\n","Epoch 265/300\n","350/350 [==============================] - ETA: 0s - loss: 0.5881 - accuracy: 0.7989\n","Epoch 265: val_accuracy did not improve from 0.75000\n","\n","Epoch 265: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.5881 - accuracy: 0.7989 - val_loss: 0.9659 - val_accuracy: 0.7140\n","Epoch 266/300\n","350/350 [==============================] - ETA: 0s - loss: 0.5902 - accuracy: 0.7946\n","Epoch 266: val_accuracy did not improve from 0.75000\n","\n","Epoch 266: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.5902 - accuracy: 0.7946 - val_loss: 1.0872 - val_accuracy: 0.7097\n","Epoch 267/300\n","350/350 [==============================] - ETA: 0s - loss: 0.5831 - accuracy: 0.8043\n","Epoch 267: val_accuracy did not improve from 0.75000\n","\n","Epoch 267: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5831 - accuracy: 0.8043 - val_loss: 1.0186 - val_accuracy: 0.6992\n","Epoch 268/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6045 - accuracy: 0.7985\n","Epoch 268: val_accuracy did not improve from 0.75000\n","\n","Epoch 268: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6046 - accuracy: 0.7987 - val_loss: 1.0317 - val_accuracy: 0.7331\n","Epoch 269/300\n","350/350 [==============================] - ETA: 0s - loss: 0.5825 - accuracy: 0.8050\n","Epoch 269: val_accuracy did not improve from 0.75000\n","\n","Epoch 269: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5825 - accuracy: 0.8050 - val_loss: 0.9505 - val_accuracy: 0.7246\n","Epoch 270/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.5788 - accuracy: 0.8039\n","Epoch 270: val_accuracy did not improve from 0.75000\n","\n","Epoch 270: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5781 - accuracy: 0.8041 - val_loss: 1.0120 - val_accuracy: 0.7140\n","Epoch 271/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6167 - accuracy: 0.7886\n","Epoch 271: val_accuracy did not improve from 0.75000\n","\n","Epoch 271: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6170 - accuracy: 0.7887 - val_loss: 0.9956 - val_accuracy: 0.7055\n","Epoch 272/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6090 - accuracy: 0.7937\n","Epoch 272: val_accuracy did not improve from 0.75000\n","\n","Epoch 272: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6084 - accuracy: 0.7939 - val_loss: 1.1666 - val_accuracy: 0.6589\n","Epoch 273/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.5956 - accuracy: 0.7992\n","Epoch 273: val_accuracy did not improve from 0.75000\n","\n","Epoch 273: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5955 - accuracy: 0.7991 - val_loss: 1.1213 - val_accuracy: 0.6992\n","Epoch 274/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6121 - accuracy: 0.7959\n","Epoch 274: val_accuracy did not improve from 0.75000\n","\n","Epoch 274: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6121 - accuracy: 0.7959 - val_loss: 0.9022 - val_accuracy: 0.7225\n","Epoch 275/300\n","350/350 [==============================] - ETA: 0s - loss: 0.5762 - accuracy: 0.8046\n","Epoch 275: val_accuracy did not improve from 0.75000\n","\n","Epoch 275: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5762 - accuracy: 0.8046 - val_loss: 1.0907 - val_accuracy: 0.6949\n","Epoch 276/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.6038 - accuracy: 0.7974\n","Epoch 276: val_accuracy did not improve from 0.75000\n","\n","Epoch 276: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6026 - accuracy: 0.7977 - val_loss: 1.0918 - val_accuracy: 0.6949\n","Epoch 277/300\n","350/350 [==============================] - ETA: 0s - loss: 0.5919 - accuracy: 0.8061\n","Epoch 277: val_accuracy did not improve from 0.75000\n","\n","Epoch 277: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.5919 - accuracy: 0.8061 - val_loss: 0.9357 - val_accuracy: 0.7246\n","Epoch 278/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6221 - accuracy: 0.7884\n","Epoch 278: val_accuracy did not improve from 0.75000\n","\n","Epoch 278: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6221 - accuracy: 0.7884 - val_loss: 1.1394 - val_accuracy: 0.6822\n","Epoch 279/300\n","350/350 [==============================] - ETA: 0s - loss: 0.5882 - accuracy: 0.8045\n","Epoch 279: val_accuracy did not improve from 0.75000\n","\n","Epoch 279: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5882 - accuracy: 0.8045 - val_loss: 1.0054 - val_accuracy: 0.7013\n","Epoch 280/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.5800 - accuracy: 0.7947\n","Epoch 280: val_accuracy did not improve from 0.75000\n","\n","Epoch 280: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5795 - accuracy: 0.7950 - val_loss: 1.0463 - val_accuracy: 0.7394\n","Epoch 281/300\n","350/350 [==============================] - ETA: 0s - loss: 0.5897 - accuracy: 0.7961\n","Epoch 281: val_accuracy did not improve from 0.75000\n","\n","Epoch 281: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5897 - accuracy: 0.7961 - val_loss: 0.9984 - val_accuracy: 0.7436\n","Epoch 282/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.5933 - accuracy: 0.8007\n","Epoch 282: val_accuracy did not improve from 0.75000\n","\n","Epoch 282: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.5935 - accuracy: 0.8007 - val_loss: 1.0050 - val_accuracy: 0.7182\n","Epoch 283/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.5849 - accuracy: 0.8042\n","Epoch 283: val_accuracy did not improve from 0.75000\n","\n","Epoch 283: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5863 - accuracy: 0.8039 - val_loss: 1.0012 - val_accuracy: 0.6992\n","Epoch 284/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.5824 - accuracy: 0.8048\n","Epoch 284: val_accuracy did not improve from 0.75000\n","\n","Epoch 284: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5823 - accuracy: 0.8048 - val_loss: 0.9413 - val_accuracy: 0.7182\n","Epoch 285/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6002 - accuracy: 0.8018\n","Epoch 285: val_accuracy did not improve from 0.75000\n","\n","Epoch 285: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5994 - accuracy: 0.8020 - val_loss: 0.8989 - val_accuracy: 0.7140\n","Epoch 286/300\n","347/350 [============================>.] - ETA: 0s - loss: 0.5978 - accuracy: 0.8037\n","Epoch 286: val_accuracy did not improve from 0.75000\n","\n","Epoch 286: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5955 - accuracy: 0.8041 - val_loss: 1.0142 - val_accuracy: 0.7076\n","Epoch 287/300\n","350/350 [==============================] - ETA: 0s - loss: 0.7381 - accuracy: 0.7477\n","Epoch 287: val_accuracy did not improve from 0.75000\n","\n","Epoch 287: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.7381 - accuracy: 0.7477 - val_loss: 1.0033 - val_accuracy: 0.7225\n","Epoch 288/300\n","350/350 [==============================] - ETA: 0s - loss: 0.6465 - accuracy: 0.7816\n","Epoch 288: val_accuracy did not improve from 0.75000\n","\n","Epoch 288: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6465 - accuracy: 0.7816 - val_loss: 1.1848 - val_accuracy: 0.6928\n","Epoch 289/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.5941 - accuracy: 0.7982\n","Epoch 289: val_accuracy did not improve from 0.75000\n","\n","Epoch 289: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5935 - accuracy: 0.7982 - val_loss: 1.0153 - val_accuracy: 0.7203\n","Epoch 290/300\n","350/350 [==============================] - ETA: 0s - loss: 0.5873 - accuracy: 0.8029\n","Epoch 290: val_accuracy did not improve from 0.75000\n","\n","Epoch 290: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5873 - accuracy: 0.8029 - val_loss: 1.0470 - val_accuracy: 0.7161\n","Epoch 291/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.5889 - accuracy: 0.8032\n","Epoch 291: val_accuracy did not improve from 0.75000\n","\n","Epoch 291: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5882 - accuracy: 0.8032 - val_loss: 1.0462 - val_accuracy: 0.7034\n","Epoch 292/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6037 - accuracy: 0.7963\n","Epoch 292: val_accuracy did not improve from 0.75000\n","\n","Epoch 292: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6041 - accuracy: 0.7961 - val_loss: 1.0546 - val_accuracy: 0.7140\n","Epoch 293/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6278 - accuracy: 0.7827\n","Epoch 293: val_accuracy did not improve from 0.75000\n","\n","Epoch 293: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6270 - accuracy: 0.7829 - val_loss: 1.0597 - val_accuracy: 0.7055\n","Epoch 294/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.5986 - accuracy: 0.7957\n","Epoch 294: val_accuracy did not improve from 0.75000\n","\n","Epoch 294: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5980 - accuracy: 0.7955 - val_loss: 1.3622 - val_accuracy: 0.6547\n","Epoch 295/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6432 - accuracy: 0.7911\n","Epoch 295: val_accuracy did not improve from 0.75000\n","\n","Epoch 295: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6426 - accuracy: 0.7911 - val_loss: 1.0533 - val_accuracy: 0.7161\n","Epoch 296/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.5864 - accuracy: 0.7953\n","Epoch 296: val_accuracy did not improve from 0.75000\n","\n","Epoch 296: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5871 - accuracy: 0.7950 - val_loss: 1.0558 - val_accuracy: 0.7034\n","Epoch 297/300\n","348/350 [============================>.] - ETA: 0s - loss: 0.6226 - accuracy: 0.7938\n","Epoch 297: val_accuracy did not improve from 0.75000\n","\n","Epoch 297: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 18ms/step - loss: 0.6217 - accuracy: 0.7937 - val_loss: 0.9960 - val_accuracy: 0.7331\n","Epoch 298/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.6052 - accuracy: 0.7932\n","Epoch 298: val_accuracy did not improve from 0.75000\n","\n","Epoch 298: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.6055 - accuracy: 0.7932 - val_loss: 1.0892 - val_accuracy: 0.6864\n","Epoch 299/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.5839 - accuracy: 0.8018\n","Epoch 299: val_accuracy did not improve from 0.75000\n","\n","Epoch 299: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5827 - accuracy: 0.8021 - val_loss: 1.0957 - val_accuracy: 0.6949\n","Epoch 300/300\n","349/350 [============================>.] - ETA: 0s - loss: 0.5815 - accuracy: 0.8012\n","Epoch 300: val_accuracy did not improve from 0.75000\n","\n","Epoch 300: val_loss did not improve from 0.86079\n","350/350 [==============================] - 6s 17ms/step - loss: 0.5811 - accuracy: 0.8016 - val_loss: 1.0672 - val_accuracy: 0.7119\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["from numpy.testing import assert_allclose\n","from keras.models import Sequential, load_model\n","from keras.callbacks import ModelCheckpoint\n","\n","dataPath = \"/kaggle/input/fastfood/FastFood\"\n","\n","batchSize = 16\n","# imgHeight = 128\n","# imgWidth = 128\n","\n","imgHeight = 150\n","imgWidth = 150\n","\n","\n","trainDataset = tf.keras.utils.image_dataset_from_directory(\n","  dataPath,\n","  validation_split=0.2,\n","  subset=\"training\",\n","  # color_mode = \"grayscale\",\n","  color_mode = \"rgb\",\n","  shuffle = True,\n","  seed=123,\n","  image_size=(imgHeight, imgWidth),\n","  batch_size=batchSize)\n","\n","valDataset = tf.keras.utils.image_dataset_from_directory(\n","  dataPath,\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  # color_mode = \"grayscale\",\n","  color_mode = \"rgb\",\n","  shuffle = True,\n","  seed=123,\n","  image_size=(imgHeight, imgWidth),\n","  batch_size=batchSize)\n","\n","valBatch = tf.data.experimental.cardinality(valDataset)\n","testDataset = valDataset.take((2*valBatch) // 3)\n","valDataset = valDataset.skip((2*valBatch) // 3)\n","\n","classNames = trainDataset.class_names\n","print(classNames)\n","\n","normalization_layer = tf.keras.layers.Rescaling(1./255)\n","trainDataset = trainDataset.map(lambda x, y: (normalization_layer(x), y))\n","\n","# Reference to normalise test dataset as well: https://datascience.stackexchange.com/questions/27615/should-we-apply-normalization-to-test-data-as-well\n","valDataset = valDataset.map(lambda x, y: (normalization_layer(x), y))\n","testDataset = testDataset.map(lambda x, y: (normalization_layer(x), y))\n","\n","AUTOTUNE = tf.data.AUTOTUNE\n","\n","# trainDataset = trainDataset.cache().prefetch(buffer_size=AUTOTUNE)\n","\n","trainDataset = trainDataset.cache().prefetch(buffer_size=AUTOTUNE)\n","valDataset = valDataset.cache().prefetch(buffer_size=AUTOTUNE)\n","\n","numClasses = 7\n","model = tf.keras.Sequential([\n","    tf.keras.layers.RandomFlip('horizontal', seed=20231206),\n","    tf.keras.layers.RandomRotation(0.2, seed=20231206),\n","    tf.keras.layers.RandomCrop(128, 128, seed=20231206),\n","    tf.keras.layers.RandomTranslation(0.2, 0.2, seed=20231206),\n","    tf.keras.layers.RandomZoom(0.1, seed=20231206),\n","    tf.keras.layers.Conv2D(32, 3, activation='relu', padding=\"same\"),\n","    tf.keras.layers.MaxPooling2D(),\n","    tf.keras.layers.Conv2D(64, 3, activation='relu', padding=\"same\"),\n","    tf.keras.layers.MaxPooling2D(),\n","    tf.keras.layers.Conv2D(128, 3, activation='relu', padding=\"same\"),\n","    tf.keras.layers.MaxPooling2D(),\n","    tf.keras.layers.Conv2D(256, 3, activation='relu', padding=\"same\"),\n","    tf.keras.layers.Conv2D(256, 3, activation='relu', padding=\"same\"),\n","    tf.keras.layers.MaxPooling2D(),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(256, activation='relu'),\n","    tf.keras.layers.Dropout(0.2),\n","    tf.keras.layers.Dense(numClasses)\n","])\n","\n","learningRate = 1e-3\n","\n","model.compile(\n","  optimizer=tf.keras.optimizers.Adam(learningRate),\n","  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","  metrics=['accuracy'])\n","\n","\n","# Reference to save and load model to continue training: https://stackoverflow.com/questions/45393429/keras-how-to-save-model-and-continue-training\n","\n","# define the checkpoint\n","filepath1 = \"/kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\"\n","\n","# To save current model in execution\n","checkpointCallback = ModelCheckpoint(filepath1, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n","# callbacksList = [checkpoint]\n","\n","# define the checkpoint\n","filepath2 = \"/kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\"\n","\n","# To save current model in execution\n","checkpointCallbackForLoss = ModelCheckpoint(filepath2, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","\n","history = model.fit(\n","  trainDataset,\n","  validation_data=valDataset,\n","  epochs=300,\n","  callbacks=[checkpointCallback, checkpointCallbackForLoss]\n",")\n","\n","# Plot training and validation loss curves\n","import matplotlib.pyplot as plt\n","plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Found 7000 files belonging to 7 classes.\n","# Using 5600 files for training.\n","# Found 7000 files belonging to 7 classes.\n","# Using 1400 files for validation.\n","# ['chicken_wings', 'fish_and_chips', 'french_fries', 'hamburger', 'ice_cream', 'onion_rings', 'pizza']\n","# Epoch 1/300\n","# 350/350 [==============================] - ETA: 0s - loss: 1.8604 - accuracy: 0.2288\n","# Epoch 1: val_accuracy improved from -inf to 0.25000, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 1: val_loss improved from inf to 1.81652, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 20s 44ms/step - loss: 1.8604 - accuracy: 0.2288 - val_loss: 1.8165 - val_accuracy: 0.2500\n","# Epoch 2/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 1.8088 - accuracy: 0.2570\n","# Epoch 2: val_accuracy improved from 0.25000 to 0.25847, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 2: val_loss improved from 1.81652 to 1.77928, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 13s 36ms/step - loss: 1.8096 - accuracy: 0.2566 - val_loss: 1.7793 - val_accuracy: 0.2585\n","# Epoch 3/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 1.7755 - accuracy: 0.2822\n","# Epoch 3: val_accuracy improved from 0.25847 to 0.29025, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 3: val_loss improved from 1.77928 to 1.74486, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 13s 38ms/step - loss: 1.7751 - accuracy: 0.2827 - val_loss: 1.7449 - val_accuracy: 0.2903\n","# Epoch 4/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 1.7483 - accuracy: 0.3030\n","# Epoch 4: val_accuracy improved from 0.29025 to 0.29873, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 4: val_loss improved from 1.74486 to 1.71775, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 13s 36ms/step - loss: 1.7483 - accuracy: 0.3032 - val_loss: 1.7178 - val_accuracy: 0.2987\n","# Epoch 5/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 1.6984 - accuracy: 0.3278\n","# Epoch 5: val_accuracy improved from 0.29873 to 0.35805, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 5: val_loss improved from 1.71775 to 1.65237, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 13s 38ms/step - loss: 1.6988 - accuracy: 0.3275 - val_loss: 1.6524 - val_accuracy: 0.3581\n","# Epoch 6/300\n","# 350/350 [==============================] - ETA: 0s - loss: 1.6384 - accuracy: 0.3620\n","# Epoch 6: val_accuracy improved from 0.35805 to 0.43432, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 6: val_loss improved from 1.65237 to 1.51166, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 13s 36ms/step - loss: 1.6384 - accuracy: 0.3620 - val_loss: 1.5117 - val_accuracy: 0.4343\n","# Epoch 7/300\n","# 350/350 [==============================] - ETA: 0s - loss: 1.5926 - accuracy: 0.3934\n","# Epoch 7: val_accuracy did not improve from 0.43432\n","\n","# Epoch 7: val_loss improved from 1.51166 to 1.45349, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 10s 27ms/step - loss: 1.5926 - accuracy: 0.3934 - val_loss: 1.4535 - val_accuracy: 0.4301\n","# Epoch 8/300\n","# 350/350 [==============================] - ETA: 0s - loss: 1.5385 - accuracy: 0.4175\n","# Epoch 8: val_accuracy improved from 0.43432 to 0.47881, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 8: val_loss improved from 1.45349 to 1.40376, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 13s 36ms/step - loss: 1.5385 - accuracy: 0.4175 - val_loss: 1.4038 - val_accuracy: 0.4788\n","# Epoch 9/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 1.5169 - accuracy: 0.4239\n","# Epoch 9: val_accuracy did not improve from 0.47881\n","\n","# Epoch 9: val_loss did not improve from 1.40376\n","# 350/350 [==============================] - 6s 17ms/step - loss: 1.5170 - accuracy: 0.4238 - val_loss: 1.4294 - val_accuracy: 0.4555\n","# Epoch 10/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 1.4649 - accuracy: 0.4611\n","# Epoch 10: val_accuracy improved from 0.47881 to 0.48093, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 10: val_loss improved from 1.40376 to 1.39875, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 14s 39ms/step - loss: 1.4648 - accuracy: 0.4620 - val_loss: 1.3987 - val_accuracy: 0.4809\n","# Epoch 11/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 1.4276 - accuracy: 0.4626\n","# Epoch 11: val_accuracy improved from 0.48093 to 0.54237, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 11: val_loss improved from 1.39875 to 1.28047, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 13s 36ms/step - loss: 1.4276 - accuracy: 0.4629 - val_loss: 1.2805 - val_accuracy: 0.5424\n","# Epoch 12/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 1.3755 - accuracy: 0.4960\n","# Epoch 12: val_accuracy did not improve from 0.54237\n","\n","# Epoch 12: val_loss did not improve from 1.28047\n","# 350/350 [==============================] - 6s 17ms/step - loss: 1.3762 - accuracy: 0.4963 - val_loss: 1.3790 - val_accuracy: 0.5106\n","# Epoch 13/300\n","# 350/350 [==============================] - ETA: 0s - loss: 1.3683 - accuracy: 0.5013\n","# Epoch 13: val_accuracy did not improve from 0.54237\n","\n","# Epoch 13: val_loss improved from 1.28047 to 1.26027, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 9s 26ms/step - loss: 1.3683 - accuracy: 0.5013 - val_loss: 1.2603 - val_accuracy: 0.5297\n","# Epoch 14/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 1.3305 - accuracy: 0.5057\n","# Epoch 14: val_accuracy improved from 0.54237 to 0.56356, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 14: val_loss did not improve from 1.26027\n","# 350/350 [==============================] - 9s 27ms/step - loss: 1.3307 - accuracy: 0.5057 - val_loss: 1.2729 - val_accuracy: 0.5636\n","# Epoch 15/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 1.2803 - accuracy: 0.5296\n","# Epoch 15: val_accuracy did not improve from 0.56356\n","\n","# Epoch 15: val_loss improved from 1.26027 to 1.21042, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 9s 27ms/step - loss: 1.2822 - accuracy: 0.5289 - val_loss: 1.2104 - val_accuracy: 0.5508\n","# Epoch 16/300\n","# 350/350 [==============================] - ETA: 0s - loss: 1.2807 - accuracy: 0.5286\n","# Epoch 16: val_accuracy did not improve from 0.56356\n","\n","# Epoch 16: val_loss improved from 1.21042 to 1.20235, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 10s 30ms/step - loss: 1.2807 - accuracy: 0.5286 - val_loss: 1.2024 - val_accuracy: 0.5636\n","# Epoch 17/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 1.2412 - accuracy: 0.5431\n","# Epoch 17: val_accuracy did not improve from 0.56356\n","\n","# Epoch 17: val_loss did not improve from 1.20235\n","# 350/350 [==============================] - 6s 18ms/step - loss: 1.2414 - accuracy: 0.5430 - val_loss: 1.2246 - val_accuracy: 0.5508\n","# Epoch 18/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 1.2352 - accuracy: 0.5393\n","# Epoch 18: val_accuracy improved from 0.56356 to 0.56568, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 18: val_loss did not improve from 1.20235\n","# 350/350 [==============================] - 9s 27ms/step - loss: 1.2358 - accuracy: 0.5395 - val_loss: 1.2336 - val_accuracy: 0.5657\n","# Epoch 19/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 1.2080 - accuracy: 0.5564\n","# Epoch 19: val_accuracy did not improve from 0.56568\n","\n","# Epoch 19: val_loss did not improve from 1.20235\n","# 350/350 [==============================] - 6s 18ms/step - loss: 1.2097 - accuracy: 0.5559 - val_loss: 1.2212 - val_accuracy: 0.5339\n","# Epoch 20/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 1.1907 - accuracy: 0.5652\n","# Epoch 20: val_accuracy did not improve from 0.56568\n","\n","# Epoch 20: val_loss did not improve from 1.20235\n","# 350/350 [==============================] - 6s 17ms/step - loss: 1.1922 - accuracy: 0.5650 - val_loss: 1.3522 - val_accuracy: 0.5106\n","# Epoch 21/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 1.1817 - accuracy: 0.5621\n","# Epoch 21: val_accuracy improved from 0.56568 to 0.57627, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 21: val_loss improved from 1.20235 to 1.16806, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 13s 36ms/step - loss: 1.1822 - accuracy: 0.5620 - val_loss: 1.1681 - val_accuracy: 0.5763\n","# Epoch 22/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 1.1678 - accuracy: 0.5795\n","# Epoch 22: val_accuracy improved from 0.57627 to 0.59534, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 22: val_loss did not improve from 1.16806\n","# 350/350 [==============================] - 9s 27ms/step - loss: 1.1674 - accuracy: 0.5796 - val_loss: 1.2034 - val_accuracy: 0.5953\n","# Epoch 23/300\n","# 350/350 [==============================] - ETA: 0s - loss: 1.1553 - accuracy: 0.5845\n","# Epoch 23: val_accuracy did not improve from 0.59534\n","\n","# Epoch 23: val_loss improved from 1.16806 to 1.16638, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 9s 27ms/step - loss: 1.1553 - accuracy: 0.5845 - val_loss: 1.1664 - val_accuracy: 0.5953\n","# Epoch 24/300\n","# 350/350 [==============================] - ETA: 0s - loss: 1.1476 - accuracy: 0.5814\n","# Epoch 24: val_accuracy did not improve from 0.59534\n","\n","# Epoch 24: val_loss did not improve from 1.16638\n","# 350/350 [==============================] - 6s 18ms/step - loss: 1.1476 - accuracy: 0.5814 - val_loss: 1.1891 - val_accuracy: 0.5742\n","# Epoch 25/300\n","# 350/350 [==============================] - ETA: 0s - loss: 1.0999 - accuracy: 0.5948\n","# Epoch 25: val_accuracy did not improve from 0.59534\n","\n","# Epoch 25: val_loss did not improve from 1.16638\n","# 350/350 [==============================] - 6s 17ms/step - loss: 1.0999 - accuracy: 0.5948 - val_loss: 1.1743 - val_accuracy: 0.5678\n","# Epoch 26/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 1.0960 - accuracy: 0.5976\n","# Epoch 26: val_accuracy did not improve from 0.59534\n","\n","# Epoch 26: val_loss did not improve from 1.16638\n","# 350/350 [==============================] - 6s 17ms/step - loss: 1.0971 - accuracy: 0.5970 - val_loss: 1.2703 - val_accuracy: 0.5699\n","# Epoch 27/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 1.0919 - accuracy: 0.6057\n","# Epoch 27: val_accuracy improved from 0.59534 to 0.61441, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 27: val_loss did not improve from 1.16638\n","# 350/350 [==============================] - 9s 26ms/step - loss: 1.0937 - accuracy: 0.6052 - val_loss: 1.1793 - val_accuracy: 0.6144\n","# Epoch 28/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 1.0732 - accuracy: 0.6155\n","# Epoch 28: val_accuracy did not improve from 0.61441\n","\n","# Epoch 28: val_loss did not improve from 1.16638\n","# 350/350 [==============================] - 6s 18ms/step - loss: 1.0740 - accuracy: 0.6154 - val_loss: 1.1773 - val_accuracy: 0.5869\n","# Epoch 29/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 1.0841 - accuracy: 0.6098\n","# Epoch 29: val_accuracy improved from 0.61441 to 0.64619, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 29: val_loss improved from 1.16638 to 1.09780, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 14s 40ms/step - loss: 1.0851 - accuracy: 0.6093 - val_loss: 1.0978 - val_accuracy: 0.6462\n","# Epoch 30/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 1.0733 - accuracy: 0.6044\n","# Epoch 30: val_accuracy did not improve from 0.64619\n","\n","# Epoch 30: val_loss improved from 1.09780 to 1.07393, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 10s 27ms/step - loss: 1.0746 - accuracy: 0.6043 - val_loss: 1.0739 - val_accuracy: 0.6186\n","# Epoch 31/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 1.0352 - accuracy: 0.6316\n","# Epoch 31: val_accuracy did not improve from 0.64619\n","\n","# Epoch 31: val_loss did not improve from 1.07393\n","# 350/350 [==============================] - 6s 17ms/step - loss: 1.0350 - accuracy: 0.6316 - val_loss: 1.1484 - val_accuracy: 0.6144\n","# Epoch 32/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 1.0581 - accuracy: 0.6185\n","# Epoch 32: val_accuracy did not improve from 0.64619\n","\n","# Epoch 32: val_loss did not improve from 1.07393\n","# 350/350 [==============================] - 6s 17ms/step - loss: 1.0607 - accuracy: 0.6179 - val_loss: 1.1104 - val_accuracy: 0.6165\n","# Epoch 33/300\n","# 350/350 [==============================] - ETA: 0s - loss: 1.0496 - accuracy: 0.6205\n","# Epoch 33: val_accuracy did not improve from 0.64619\n","\n","# Epoch 33: val_loss did not improve from 1.07393\n","# 350/350 [==============================] - 6s 17ms/step - loss: 1.0496 - accuracy: 0.6205 - val_loss: 1.0907 - val_accuracy: 0.6441\n","# Epoch 34/300\n","# 350/350 [==============================] - ETA: 0s - loss: 1.0335 - accuracy: 0.6277\n","# Epoch 34: val_accuracy did not improve from 0.64619\n","\n","# Epoch 34: val_loss did not improve from 1.07393\n","# 350/350 [==============================] - 6s 17ms/step - loss: 1.0335 - accuracy: 0.6277 - val_loss: 1.1737 - val_accuracy: 0.6102\n","# Epoch 35/300\n","# 350/350 [==============================] - ETA: 0s - loss: 1.0028 - accuracy: 0.6320\n","# Epoch 35: val_accuracy did not improve from 0.64619\n","\n","# Epoch 35: val_loss did not improve from 1.07393\n","# 350/350 [==============================] - 6s 17ms/step - loss: 1.0028 - accuracy: 0.6320 - val_loss: 1.2247 - val_accuracy: 0.5869\n","# Epoch 36/300\n","# 350/350 [==============================] - ETA: 0s - loss: 1.0086 - accuracy: 0.6371\n","# Epoch 36: val_accuracy did not improve from 0.64619\n","\n","# Epoch 36: val_loss improved from 1.07393 to 1.04797, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 9s 27ms/step - loss: 1.0086 - accuracy: 0.6371 - val_loss: 1.0480 - val_accuracy: 0.6356\n","# Epoch 37/300\n","# 350/350 [==============================] - ETA: 0s - loss: 1.0047 - accuracy: 0.6418\n","# Epoch 37: val_accuracy did not improve from 0.64619\n","\n","# Epoch 37: val_loss did not improve from 1.04797\n","# 350/350 [==============================] - 6s 18ms/step - loss: 1.0047 - accuracy: 0.6418 - val_loss: 1.2284 - val_accuracy: 0.5636\n","# Epoch 38/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.9945 - accuracy: 0.6426\n","# Epoch 38: val_accuracy did not improve from 0.64619\n","\n","# Epoch 38: val_loss did not improve from 1.04797\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.9943 - accuracy: 0.6429 - val_loss: 1.0577 - val_accuracy: 0.6441\n","# Epoch 39/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.9754 - accuracy: 0.6557\n","# Epoch 39: val_accuracy did not improve from 0.64619\n","\n","# Epoch 39: val_loss did not improve from 1.04797\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.9760 - accuracy: 0.6552 - val_loss: 1.2892 - val_accuracy: 0.5869\n","# Epoch 40/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.9766 - accuracy: 0.6507\n","# Epoch 40: val_accuracy improved from 0.64619 to 0.64831, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 40: val_loss did not improve from 1.04797\n","# 350/350 [==============================] - 9s 27ms/step - loss: 0.9782 - accuracy: 0.6511 - val_loss: 1.0689 - val_accuracy: 0.6483\n","# Epoch 41/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.9728 - accuracy: 0.6494\n","# Epoch 41: val_accuracy did not improve from 0.64831\n","\n","# Epoch 41: val_loss did not improve from 1.04797\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.9722 - accuracy: 0.6496 - val_loss: 1.1100 - val_accuracy: 0.6314\n","# Epoch 42/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.9435 - accuracy: 0.6597\n","# Epoch 42: val_accuracy did not improve from 0.64831\n","\n","# Epoch 42: val_loss did not improve from 1.04797\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.9444 - accuracy: 0.6596 - val_loss: 1.0900 - val_accuracy: 0.6462\n","# Epoch 43/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.9467 - accuracy: 0.6622\n","# Epoch 43: val_accuracy improved from 0.64831 to 0.66525, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 43: val_loss improved from 1.04797 to 0.99045, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 13s 36ms/step - loss: 0.9465 - accuracy: 0.6618 - val_loss: 0.9905 - val_accuracy: 0.6653\n","# Epoch 44/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.9428 - accuracy: 0.6564\n","# Epoch 44: val_accuracy did not improve from 0.66525\n","\n","# Epoch 44: val_loss did not improve from 0.99045\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.9428 - accuracy: 0.6564 - val_loss: 1.0859 - val_accuracy: 0.6483\n","# Epoch 45/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.9428 - accuracy: 0.6611\n","# Epoch 45: val_accuracy did not improve from 0.66525\n","\n","# Epoch 45: val_loss did not improve from 0.99045\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.9428 - accuracy: 0.6611 - val_loss: 1.1864 - val_accuracy: 0.6165\n","# Epoch 46/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.9403 - accuracy: 0.6641\n","# Epoch 46: val_accuracy did not improve from 0.66525\n","\n","# Epoch 46: val_loss did not improve from 0.99045\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.9403 - accuracy: 0.6641 - val_loss: 1.0550 - val_accuracy: 0.6398\n","# Epoch 47/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.9248 - accuracy: 0.6677\n","# Epoch 47: val_accuracy did not improve from 0.66525\n","\n","# Epoch 47: val_loss did not improve from 0.99045\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.9249 - accuracy: 0.6680 - val_loss: 1.0675 - val_accuracy: 0.6589\n","# Epoch 48/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.9341 - accuracy: 0.6610\n","# Epoch 48: val_accuracy did not improve from 0.66525\n","\n","# Epoch 48: val_loss did not improve from 0.99045\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.9341 - accuracy: 0.6612 - val_loss: 1.1377 - val_accuracy: 0.6525\n","# Epoch 49/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.9229 - accuracy: 0.6720\n","# Epoch 49: val_accuracy did not improve from 0.66525\n","\n","# Epoch 49: val_loss did not improve from 0.99045\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.9227 - accuracy: 0.6723 - val_loss: 1.0687 - val_accuracy: 0.6547\n","# Epoch 50/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.9023 - accuracy: 0.6839\n","# Epoch 50: val_accuracy did not improve from 0.66525\n","\n","# Epoch 50: val_loss did not improve from 0.99045\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.9038 - accuracy: 0.6836 - val_loss: 1.1593 - val_accuracy: 0.6398\n","# Epoch 51/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.9055 - accuracy: 0.6747\n","# Epoch 51: val_accuracy improved from 0.66525 to 0.68008, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 51: val_loss did not improve from 0.99045\n","# 350/350 [==============================] - 9s 27ms/step - loss: 0.9061 - accuracy: 0.6752 - val_loss: 1.0065 - val_accuracy: 0.6801\n","# Epoch 52/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.8919 - accuracy: 0.6818\n","# Epoch 52: val_accuracy did not improve from 0.68008\n","\n","# Epoch 52: val_loss did not improve from 0.99045\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.8919 - accuracy: 0.6818 - val_loss: 1.1897 - val_accuracy: 0.6462\n","# Epoch 53/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.8932 - accuracy: 0.6798\n","# Epoch 53: val_accuracy did not improve from 0.68008\n","\n","# Epoch 53: val_loss did not improve from 0.99045\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.8928 - accuracy: 0.6802 - val_loss: 1.2750 - val_accuracy: 0.6165\n","# Epoch 54/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.8920 - accuracy: 0.6855\n","# Epoch 54: val_accuracy did not improve from 0.68008\n","\n","# Epoch 54: val_loss did not improve from 0.99045\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.8920 - accuracy: 0.6855 - val_loss: 1.1178 - val_accuracy: 0.6208\n","# Epoch 55/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.9029 - accuracy: 0.6812\n","# Epoch 55: val_accuracy did not improve from 0.68008\n","\n","# Epoch 55: val_loss did not improve from 0.99045\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.9026 - accuracy: 0.6814 - val_loss: 1.1965 - val_accuracy: 0.6123\n","# Epoch 56/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.9055 - accuracy: 0.6777\n","# Epoch 56: val_accuracy did not improve from 0.68008\n","\n","# Epoch 56: val_loss did not improve from 0.99045\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.9055 - accuracy: 0.6777 - val_loss: 1.1704 - val_accuracy: 0.6292\n","# Epoch 57/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.8819 - accuracy: 0.6889\n","# Epoch 57: val_accuracy did not improve from 0.68008\n","\n","# Epoch 57: val_loss did not improve from 0.99045\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.8819 - accuracy: 0.6889 - val_loss: 1.1577 - val_accuracy: 0.6441\n","# Epoch 58/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.8788 - accuracy: 0.6941\n","# Epoch 58: val_accuracy did not improve from 0.68008\n","\n","# Epoch 58: val_loss did not improve from 0.99045\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.8816 - accuracy: 0.6936 - val_loss: 1.1005 - val_accuracy: 0.6462\n","# Epoch 59/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.8643 - accuracy: 0.6943\n","# Epoch 59: val_accuracy did not improve from 0.68008\n","\n","# Epoch 59: val_loss did not improve from 0.99045\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.8648 - accuracy: 0.6941 - val_loss: 1.1089 - val_accuracy: 0.6462\n","# Epoch 60/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.8623 - accuracy: 0.6927\n","# Epoch 60: val_accuracy did not improve from 0.68008\n","\n","# Epoch 60: val_loss did not improve from 0.99045\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.8623 - accuracy: 0.6927 - val_loss: 1.0331 - val_accuracy: 0.6737\n","# Epoch 61/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.8643 - accuracy: 0.6957\n","# Epoch 61: val_accuracy did not improve from 0.68008\n","\n","# Epoch 61: val_loss improved from 0.99045 to 0.95968, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 10s 27ms/step - loss: 0.8643 - accuracy: 0.6957 - val_loss: 0.9597 - val_accuracy: 0.6631\n","# Epoch 62/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.8489 - accuracy: 0.7004\n","# Epoch 62: val_accuracy did not improve from 0.68008\n","\n","# Epoch 62: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.8504 - accuracy: 0.7000 - val_loss: 1.0152 - val_accuracy: 0.6610\n","# Epoch 63/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.8787 - accuracy: 0.6862\n","# Epoch 63: val_accuracy did not improve from 0.68008\n","\n","# Epoch 63: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.8804 - accuracy: 0.6855 - val_loss: 1.2117 - val_accuracy: 0.6165\n","# Epoch 64/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.8506 - accuracy: 0.7030\n","# Epoch 64: val_accuracy did not improve from 0.68008\n","\n","# Epoch 64: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.8506 - accuracy: 0.7030 - val_loss: 1.0627 - val_accuracy: 0.6398\n","# Epoch 65/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.8385 - accuracy: 0.7058\n","# Epoch 65: val_accuracy did not improve from 0.68008\n","\n","# Epoch 65: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.8386 - accuracy: 0.7057 - val_loss: 1.2009 - val_accuracy: 0.6504\n","# Epoch 66/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.8245 - accuracy: 0.7097\n","# Epoch 66: val_accuracy did not improve from 0.68008\n","\n","# Epoch 66: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.8245 - accuracy: 0.7098 - val_loss: 1.0569 - val_accuracy: 0.6674\n","# Epoch 67/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.8182 - accuracy: 0.7061\n","# Epoch 67: val_accuracy did not improve from 0.68008\n","\n","# Epoch 67: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.8182 - accuracy: 0.7061 - val_loss: 1.0966 - val_accuracy: 0.6377\n","# Epoch 68/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.8467 - accuracy: 0.7073\n","# Epoch 68: val_accuracy did not improve from 0.68008\n","\n","# Epoch 68: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.8483 - accuracy: 0.7073 - val_loss: 1.0921 - val_accuracy: 0.6547\n","# Epoch 69/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.8216 - accuracy: 0.7120\n","# Epoch 69: val_accuracy did not improve from 0.68008\n","\n","# Epoch 69: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.8216 - accuracy: 0.7120 - val_loss: 1.1369 - val_accuracy: 0.6589\n","# Epoch 70/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.8367 - accuracy: 0.7156\n","# Epoch 70: val_accuracy did not improve from 0.68008\n","\n","# Epoch 70: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.8373 - accuracy: 0.7154 - val_loss: 1.1094 - val_accuracy: 0.6589\n","# Epoch 71/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.8258 - accuracy: 0.7113\n","# Epoch 71: val_accuracy did not improve from 0.68008\n","\n","# Epoch 71: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.8258 - accuracy: 0.7113 - val_loss: 1.0663 - val_accuracy: 0.6716\n","# Epoch 72/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.8396 - accuracy: 0.7071\n","# Epoch 72: val_accuracy did not improve from 0.68008\n","\n","# Epoch 72: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.8390 - accuracy: 0.7073 - val_loss: 1.1085 - val_accuracy: 0.6653\n","# Epoch 73/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.8210 - accuracy: 0.7152\n","# Epoch 73: val_accuracy did not improve from 0.68008\n","\n","# Epoch 73: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.8210 - accuracy: 0.7152 - val_loss: 1.1739 - val_accuracy: 0.6483\n","# Epoch 74/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.8138 - accuracy: 0.7219\n","# Epoch 74: val_accuracy did not improve from 0.68008\n","\n","# Epoch 74: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.8132 - accuracy: 0.7223 - val_loss: 1.1316 - val_accuracy: 0.6419\n","# Epoch 75/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.8062 - accuracy: 0.7173\n","# Epoch 75: val_accuracy did not improve from 0.68008\n","\n","# Epoch 75: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.8062 - accuracy: 0.7173 - val_loss: 1.1018 - val_accuracy: 0.6695\n","# Epoch 76/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.7990 - accuracy: 0.7204\n","# Epoch 76: val_accuracy did not improve from 0.68008\n","\n","# Epoch 76: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.8000 - accuracy: 0.7200 - val_loss: 1.0997 - val_accuracy: 0.6483\n","# Epoch 77/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.7869 - accuracy: 0.7232\n","# Epoch 77: val_accuracy did not improve from 0.68008\n","\n","# Epoch 77: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7876 - accuracy: 0.7229 - val_loss: 1.0930 - val_accuracy: 0.6737\n","# Epoch 78/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.7811 - accuracy: 0.7292\n","# Epoch 78: val_accuracy did not improve from 0.68008\n","\n","# Epoch 78: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7822 - accuracy: 0.7291 - val_loss: 1.1064 - val_accuracy: 0.6610\n","# Epoch 79/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.7911 - accuracy: 0.7284\n","# Epoch 79: val_accuracy did not improve from 0.68008\n","\n","# Epoch 79: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7911 - accuracy: 0.7286 - val_loss: 1.0813 - val_accuracy: 0.6483\n","# Epoch 80/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.7959 - accuracy: 0.7251\n","# Epoch 80: val_accuracy did not improve from 0.68008\n","\n","# Epoch 80: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7953 - accuracy: 0.7254 - val_loss: 1.1044 - val_accuracy: 0.6568\n","# Epoch 81/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.7758 - accuracy: 0.7282\n","# Epoch 81: val_accuracy did not improve from 0.68008\n","\n","# Epoch 81: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7747 - accuracy: 0.7284 - val_loss: 1.1315 - val_accuracy: 0.6780\n","# Epoch 82/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.7839 - accuracy: 0.7271\n","# Epoch 82: val_accuracy did not improve from 0.68008\n","\n","# Epoch 82: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7830 - accuracy: 0.7273 - val_loss: 1.1996 - val_accuracy: 0.6314\n","# Epoch 83/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.7735 - accuracy: 0.7265\n","# Epoch 83: val_accuracy did not improve from 0.68008\n","\n","# Epoch 83: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7734 - accuracy: 0.7266 - val_loss: 1.0717 - val_accuracy: 0.6737\n","# Epoch 84/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.7827 - accuracy: 0.7362\n","# Epoch 84: val_accuracy did not improve from 0.68008\n","\n","# Epoch 84: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7814 - accuracy: 0.7366 - val_loss: 1.1824 - val_accuracy: 0.6398\n","# Epoch 85/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.7994 - accuracy: 0.7209\n","# Epoch 85: val_accuracy did not improve from 0.68008\n","\n","# Epoch 85: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7994 - accuracy: 0.7209 - val_loss: 1.0504 - val_accuracy: 0.6758\n","# Epoch 86/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.7858 - accuracy: 0.7125\n","# Epoch 86: val_accuracy did not improve from 0.68008\n","\n","# Epoch 86: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7858 - accuracy: 0.7125 - val_loss: 1.1596 - val_accuracy: 0.6419\n","# Epoch 87/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.7636 - accuracy: 0.7363\n","# Epoch 87: val_accuracy did not improve from 0.68008\n","\n","# Epoch 87: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7649 - accuracy: 0.7359 - val_loss: 1.0941 - val_accuracy: 0.6674\n","# Epoch 88/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.7970 - accuracy: 0.7210\n","# Epoch 88: val_accuracy did not improve from 0.68008\n","\n","# Epoch 88: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7978 - accuracy: 0.7209 - val_loss: 1.1909 - val_accuracy: 0.6631\n","# Epoch 89/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.7590 - accuracy: 0.7360\n","# Epoch 89: val_accuracy did not improve from 0.68008\n","\n","# Epoch 89: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7583 - accuracy: 0.7364 - val_loss: 1.0749 - val_accuracy: 0.6653\n","# Epoch 90/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.7563 - accuracy: 0.7375\n","# Epoch 90: val_accuracy did not improve from 0.68008\n","\n","# Epoch 90: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7563 - accuracy: 0.7375 - val_loss: 1.0706 - val_accuracy: 0.6716\n","# Epoch 91/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.7748 - accuracy: 0.7267\n","# Epoch 91: val_accuracy did not improve from 0.68008\n","\n","# Epoch 91: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.7739 - accuracy: 0.7270 - val_loss: 1.0611 - val_accuracy: 0.6568\n","# Epoch 92/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.7540 - accuracy: 0.7362\n","# Epoch 92: val_accuracy did not improve from 0.68008\n","\n","# Epoch 92: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7549 - accuracy: 0.7359 - val_loss: 1.0671 - val_accuracy: 0.6695\n","# Epoch 93/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.7579 - accuracy: 0.7380\n","# Epoch 93: val_accuracy did not improve from 0.68008\n","\n","# Epoch 93: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7579 - accuracy: 0.7380 - val_loss: 1.0624 - val_accuracy: 0.6483\n","# Epoch 94/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.7619 - accuracy: 0.7399\n","# Epoch 94: val_accuracy did not improve from 0.68008\n","\n","# Epoch 94: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7613 - accuracy: 0.7404 - val_loss: 1.0087 - val_accuracy: 0.6674\n","# Epoch 95/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.7861 - accuracy: 0.7321\n","# Epoch 95: val_accuracy did not improve from 0.68008\n","\n","# Epoch 95: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7857 - accuracy: 0.7323 - val_loss: 1.0817 - val_accuracy: 0.6271\n","# Epoch 96/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.7523 - accuracy: 0.7387\n","# Epoch 96: val_accuracy did not improve from 0.68008\n","\n","# Epoch 96: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7513 - accuracy: 0.7391 - val_loss: 1.0229 - val_accuracy: 0.6674\n","# Epoch 97/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.7695 - accuracy: 0.7288\n","# Epoch 97: val_accuracy did not improve from 0.68008\n","\n","# Epoch 97: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7680 - accuracy: 0.7295 - val_loss: 1.0889 - val_accuracy: 0.6758\n","# Epoch 98/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.7408 - accuracy: 0.7432\n","# Epoch 98: val_accuracy did not improve from 0.68008\n","\n","# Epoch 98: val_loss did not improve from 0.95968\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7403 - accuracy: 0.7432 - val_loss: 1.1071 - val_accuracy: 0.6589\n","# Epoch 99/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.7443 - accuracy: 0.7400\n","# Epoch 99: val_accuracy improved from 0.68008 to 0.70551, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 99: val_loss improved from 0.95968 to 0.94320, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 14s 40ms/step - loss: 0.7443 - accuracy: 0.7400 - val_loss: 0.9432 - val_accuracy: 0.7055\n","# Epoch 100/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.7393 - accuracy: 0.7477\n","# Epoch 100: val_accuracy did not improve from 0.70551\n","\n","# Epoch 100: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.7390 - accuracy: 0.7479 - val_loss: 0.9856 - val_accuracy: 0.6907\n","# Epoch 101/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.7287 - accuracy: 0.7453\n","# Epoch 101: val_accuracy did not improve from 0.70551\n","\n","# Epoch 101: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7293 - accuracy: 0.7454 - val_loss: 1.0146 - val_accuracy: 0.6949\n","# Epoch 102/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.7554 - accuracy: 0.7446\n","# Epoch 102: val_accuracy did not improve from 0.70551\n","\n","# Epoch 102: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7562 - accuracy: 0.7446 - val_loss: 1.1519 - val_accuracy: 0.6504\n","# Epoch 103/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.7398 - accuracy: 0.7414\n","# Epoch 103: val_accuracy did not improve from 0.70551\n","\n","# Epoch 103: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7416 - accuracy: 0.7420 - val_loss: 1.1408 - val_accuracy: 0.6695\n","# Epoch 104/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.7430 - accuracy: 0.7401\n","# Epoch 104: val_accuracy did not improve from 0.70551\n","\n","# Epoch 104: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7431 - accuracy: 0.7404 - val_loss: 1.0341 - val_accuracy: 0.6610\n","# Epoch 105/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.7396 - accuracy: 0.7409\n","# Epoch 105: val_accuracy did not improve from 0.70551\n","\n","# Epoch 105: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.7395 - accuracy: 0.7407 - val_loss: 1.0080 - val_accuracy: 0.6568\n","# Epoch 106/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.7329 - accuracy: 0.7473\n","# Epoch 106: val_accuracy did not improve from 0.70551\n","\n","# Epoch 106: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7320 - accuracy: 0.7482 - val_loss: 1.2038 - val_accuracy: 0.6250\n","# Epoch 107/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.7238 - accuracy: 0.7496\n","# Epoch 107: val_accuracy did not improve from 0.70551\n","\n","# Epoch 107: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7243 - accuracy: 0.7496 - val_loss: 1.1110 - val_accuracy: 0.6653\n","# Epoch 108/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.7089 - accuracy: 0.7548\n","# Epoch 108: val_accuracy did not improve from 0.70551\n","\n","# Epoch 108: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7089 - accuracy: 0.7548 - val_loss: 1.2752 - val_accuracy: 0.6483\n","# Epoch 109/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.7286 - accuracy: 0.7421\n","# Epoch 109: val_accuracy did not improve from 0.70551\n","\n","# Epoch 109: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.7287 - accuracy: 0.7423 - val_loss: 1.1988 - val_accuracy: 0.6589\n","# Epoch 110/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.7212 - accuracy: 0.7486\n","# Epoch 110: val_accuracy did not improve from 0.70551\n","\n","# Epoch 110: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.7212 - accuracy: 0.7486 - val_loss: 0.9759 - val_accuracy: 0.6907\n","# Epoch 111/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.7227 - accuracy: 0.7484\n","# Epoch 111: val_accuracy did not improve from 0.70551\n","\n","# Epoch 111: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7231 - accuracy: 0.7482 - val_loss: 1.0146 - val_accuracy: 0.6674\n","# Epoch 112/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.7198 - accuracy: 0.7471\n","# Epoch 112: val_accuracy improved from 0.70551 to 0.71398, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 112: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 9s 27ms/step - loss: 0.7216 - accuracy: 0.7468 - val_loss: 0.9784 - val_accuracy: 0.7140\n","# Epoch 113/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.7121 - accuracy: 0.7543\n","# Epoch 113: val_accuracy did not improve from 0.71398\n","\n","# Epoch 113: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7139 - accuracy: 0.7539 - val_loss: 0.9830 - val_accuracy: 0.6970\n","# Epoch 114/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.7169 - accuracy: 0.7473\n","# Epoch 114: val_accuracy did not improve from 0.71398\n","\n","# Epoch 114: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7169 - accuracy: 0.7473 - val_loss: 1.1655 - val_accuracy: 0.6695\n","# Epoch 115/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.7440 - accuracy: 0.7407\n","# Epoch 115: val_accuracy did not improve from 0.71398\n","\n","# Epoch 115: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7427 - accuracy: 0.7414 - val_loss: 1.0944 - val_accuracy: 0.6864\n","# Epoch 116/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.7223 - accuracy: 0.7505\n","# Epoch 116: val_accuracy did not improve from 0.71398\n","\n","# Epoch 116: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7225 - accuracy: 0.7507 - val_loss: 1.0795 - val_accuracy: 0.6992\n","# Epoch 117/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.7354 - accuracy: 0.7518\n","# Epoch 117: val_accuracy did not improve from 0.71398\n","\n","# Epoch 117: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7354 - accuracy: 0.7518 - val_loss: 1.0831 - val_accuracy: 0.6758\n","# Epoch 118/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.7352 - accuracy: 0.7434\n","# Epoch 118: val_accuracy did not improve from 0.71398\n","\n","# Epoch 118: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7347 - accuracy: 0.7434 - val_loss: 0.9623 - val_accuracy: 0.6801\n","# Epoch 119/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.7294 - accuracy: 0.7468\n","# Epoch 119: val_accuracy did not improve from 0.71398\n","\n","# Epoch 119: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7281 - accuracy: 0.7475 - val_loss: 0.9480 - val_accuracy: 0.6907\n","# Epoch 120/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.7053 - accuracy: 0.7522\n","# Epoch 120: val_accuracy did not improve from 0.71398\n","\n","# Epoch 120: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7075 - accuracy: 0.7514 - val_loss: 0.9724 - val_accuracy: 0.6970\n","# Epoch 121/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.6803 - accuracy: 0.7655\n","# Epoch 121: val_accuracy did not improve from 0.71398\n","\n","# Epoch 121: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6802 - accuracy: 0.7657 - val_loss: 1.0431 - val_accuracy: 0.6801\n","# Epoch 122/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.7046 - accuracy: 0.7583\n","# Epoch 122: val_accuracy did not improve from 0.71398\n","\n","# Epoch 122: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7038 - accuracy: 0.7580 - val_loss: 1.0483 - val_accuracy: 0.6801\n","# Epoch 123/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.7151 - accuracy: 0.7552\n","# Epoch 123: val_accuracy did not improve from 0.71398\n","\n","# Epoch 123: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7149 - accuracy: 0.7552 - val_loss: 1.0248 - val_accuracy: 0.7034\n","# Epoch 124/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6796 - accuracy: 0.7651\n","# Epoch 124: val_accuracy did not improve from 0.71398\n","\n","# Epoch 124: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6812 - accuracy: 0.7646 - val_loss: 1.1549 - val_accuracy: 0.6864\n","# Epoch 125/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.7169 - accuracy: 0.7434\n","# Epoch 125: val_accuracy did not improve from 0.71398\n","\n","# Epoch 125: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7164 - accuracy: 0.7437 - val_loss: 0.9794 - val_accuracy: 0.7013\n","# Epoch 126/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.7132 - accuracy: 0.7530\n","# Epoch 126: val_accuracy did not improve from 0.71398\n","\n","# Epoch 126: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7133 - accuracy: 0.7529 - val_loss: 1.0046 - val_accuracy: 0.6992\n","# Epoch 127/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6836 - accuracy: 0.7609\n","# Epoch 127: val_accuracy did not improve from 0.71398\n","\n","# Epoch 127: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6827 - accuracy: 0.7613 - val_loss: 1.2024 - val_accuracy: 0.6589\n","# Epoch 128/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6734 - accuracy: 0.7698\n","# Epoch 128: val_accuracy did not improve from 0.71398\n","\n","# Epoch 128: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6747 - accuracy: 0.7696 - val_loss: 0.9895 - val_accuracy: 0.6716\n","# Epoch 129/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6862 - accuracy: 0.7596\n","# Epoch 129: val_accuracy did not improve from 0.71398\n","\n","# Epoch 129: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6862 - accuracy: 0.7596 - val_loss: 1.1043 - val_accuracy: 0.6737\n","# Epoch 130/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.7058 - accuracy: 0.7572\n","# Epoch 130: val_accuracy did not improve from 0.71398\n","\n","# Epoch 130: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7044 - accuracy: 0.7575 - val_loss: 1.3367 - val_accuracy: 0.6377\n","# Epoch 131/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.7097 - accuracy: 0.7559\n","# Epoch 131: val_accuracy did not improve from 0.71398\n","\n","# Epoch 131: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7103 - accuracy: 0.7557 - val_loss: 0.9608 - val_accuracy: 0.6758\n","# Epoch 132/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.7609\n","# Epoch 132: val_accuracy did not improve from 0.71398\n","\n","# Epoch 132: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6931 - accuracy: 0.7609 - val_loss: 1.0908 - val_accuracy: 0.6589\n","# Epoch 133/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6901 - accuracy: 0.7534\n","# Epoch 133: val_accuracy did not improve from 0.71398\n","\n","# Epoch 133: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6915 - accuracy: 0.7530 - val_loss: 1.1262 - val_accuracy: 0.6589\n","# Epoch 134/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6960 - accuracy: 0.7652\n","# Epoch 134: val_accuracy did not improve from 0.71398\n","\n","# Epoch 134: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6960 - accuracy: 0.7654 - val_loss: 1.0189 - val_accuracy: 0.6864\n","# Epoch 135/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6727 - accuracy: 0.7726\n","# Epoch 135: val_accuracy did not improve from 0.71398\n","\n","# Epoch 135: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6727 - accuracy: 0.7723 - val_loss: 1.1028 - val_accuracy: 0.6780\n","# Epoch 136/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6755 - accuracy: 0.7673\n","# Epoch 136: val_accuracy did not improve from 0.71398\n","\n","# Epoch 136: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6755 - accuracy: 0.7673 - val_loss: 1.1378 - val_accuracy: 0.6780\n","# Epoch 137/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6854 - accuracy: 0.7651\n","# Epoch 137: val_accuracy did not improve from 0.71398\n","\n","# Epoch 137: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6855 - accuracy: 0.7655 - val_loss: 1.0070 - val_accuracy: 0.6674\n","# Epoch 138/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6765 - accuracy: 0.7633\n","# Epoch 138: val_accuracy did not improve from 0.71398\n","\n","# Epoch 138: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6755 - accuracy: 0.7636 - val_loss: 1.0228 - val_accuracy: 0.6886\n","# Epoch 139/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.7649\n","# Epoch 139: val_accuracy did not improve from 0.71398\n","\n","# Epoch 139: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6921 - accuracy: 0.7643 - val_loss: 1.1631 - val_accuracy: 0.6949\n","# Epoch 140/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6902 - accuracy: 0.7636\n","# Epoch 140: val_accuracy did not improve from 0.71398\n","\n","# Epoch 140: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6898 - accuracy: 0.7636 - val_loss: 0.9935 - val_accuracy: 0.7097\n","# Epoch 141/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6959 - accuracy: 0.7572\n","# Epoch 141: val_accuracy did not improve from 0.71398\n","\n","# Epoch 141: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6956 - accuracy: 0.7573 - val_loss: 1.0967 - val_accuracy: 0.6695\n","# Epoch 142/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6405 - accuracy: 0.7814\n","# Epoch 142: val_accuracy did not improve from 0.71398\n","\n","# Epoch 142: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6416 - accuracy: 0.7811 - val_loss: 1.1481 - val_accuracy: 0.6907\n","# Epoch 143/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.7031 - accuracy: 0.7588\n","# Epoch 143: val_accuracy did not improve from 0.71398\n","\n","# Epoch 143: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.7021 - accuracy: 0.7593 - val_loss: 1.2114 - val_accuracy: 0.6695\n","# Epoch 144/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6661 - accuracy: 0.7736\n","# Epoch 144: val_accuracy did not improve from 0.71398\n","\n","# Epoch 144: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6661 - accuracy: 0.7736 - val_loss: 1.0606 - val_accuracy: 0.6907\n","# Epoch 145/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6704 - accuracy: 0.7710\n","# Epoch 145: val_accuracy did not improve from 0.71398\n","\n","# Epoch 145: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6701 - accuracy: 0.7711 - val_loss: 1.2348 - val_accuracy: 0.6356\n","# Epoch 146/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6617 - accuracy: 0.7638\n","# Epoch 146: val_accuracy did not improve from 0.71398\n","\n","# Epoch 146: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6617 - accuracy: 0.7638 - val_loss: 1.1305 - val_accuracy: 0.6525\n","# Epoch 147/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.6553 - accuracy: 0.7725\n","# Epoch 147: val_accuracy did not improve from 0.71398\n","\n","# Epoch 147: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6549 - accuracy: 0.7725 - val_loss: 1.0900 - val_accuracy: 0.6568\n","# Epoch 148/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6725 - accuracy: 0.7649\n","# Epoch 148: val_accuracy did not improve from 0.71398\n","\n","# Epoch 148: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6716 - accuracy: 0.7652 - val_loss: 1.0904 - val_accuracy: 0.6716\n","# Epoch 149/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6700 - accuracy: 0.7654\n","# Epoch 149: val_accuracy did not improve from 0.71398\n","\n","# Epoch 149: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6711 - accuracy: 0.7652 - val_loss: 1.0228 - val_accuracy: 0.6864\n","# Epoch 150/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6590 - accuracy: 0.7711\n","# Epoch 150: val_accuracy did not improve from 0.71398\n","\n","# Epoch 150: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6590 - accuracy: 0.7711 - val_loss: 1.0019 - val_accuracy: 0.6907\n","# Epoch 151/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6728 - accuracy: 0.7698\n","# Epoch 151: val_accuracy did not improve from 0.71398\n","\n","# Epoch 151: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6720 - accuracy: 0.7705 - val_loss: 1.1805 - val_accuracy: 0.6949\n","# Epoch 152/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6585 - accuracy: 0.7784\n","# Epoch 152: val_accuracy did not improve from 0.71398\n","\n","# Epoch 152: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6601 - accuracy: 0.7779 - val_loss: 1.0031 - val_accuracy: 0.6928\n","# Epoch 153/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6628 - accuracy: 0.7719\n","# Epoch 153: val_accuracy did not improve from 0.71398\n","\n","# Epoch 153: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6625 - accuracy: 0.7720 - val_loss: 1.1053 - val_accuracy: 0.6547\n","# Epoch 154/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6707 - accuracy: 0.7621\n","# Epoch 154: val_accuracy did not improve from 0.71398\n","\n","# Epoch 154: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6707 - accuracy: 0.7621 - val_loss: 1.1115 - val_accuracy: 0.6843\n","# Epoch 155/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6575 - accuracy: 0.7727\n","# Epoch 155: val_accuracy did not improve from 0.71398\n","\n","# Epoch 155: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6571 - accuracy: 0.7729 - val_loss: 1.0753 - val_accuracy: 0.6547\n","# Epoch 156/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6958 - accuracy: 0.7608\n","# Epoch 156: val_accuracy did not improve from 0.71398\n","\n","# Epoch 156: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6948 - accuracy: 0.7609 - val_loss: 1.1900 - val_accuracy: 0.6907\n","# Epoch 157/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.6440 - accuracy: 0.7723\n","# Epoch 157: val_accuracy improved from 0.71398 to 0.72034, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 157: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 9s 27ms/step - loss: 0.6430 - accuracy: 0.7730 - val_loss: 1.0627 - val_accuracy: 0.7203\n","# Epoch 158/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6598 - accuracy: 0.7731\n","# Epoch 158: val_accuracy did not improve from 0.72034\n","\n","# Epoch 158: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6591 - accuracy: 0.7734 - val_loss: 0.9887 - val_accuracy: 0.7055\n","# Epoch 159/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6767 - accuracy: 0.7677\n","# Epoch 159: val_accuracy did not improve from 0.72034\n","\n","# Epoch 159: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6767 - accuracy: 0.7677 - val_loss: 1.0443 - val_accuracy: 0.7055\n","# Epoch 160/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.6839 - accuracy: 0.7695\n","# Epoch 160: val_accuracy did not improve from 0.72034\n","\n","# Epoch 160: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6829 - accuracy: 0.7696 - val_loss: 1.0655 - val_accuracy: 0.6758\n","# Epoch 161/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6696 - accuracy: 0.7663\n","# Epoch 161: val_accuracy did not improve from 0.72034\n","\n","# Epoch 161: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6694 - accuracy: 0.7664 - val_loss: 1.1604 - val_accuracy: 0.6716\n","# Epoch 162/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6627 - accuracy: 0.7795\n","# Epoch 162: val_accuracy did not improve from 0.72034\n","\n","# Epoch 162: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6625 - accuracy: 0.7798 - val_loss: 1.0421 - val_accuracy: 0.7097\n","# Epoch 163/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6623 - accuracy: 0.7728\n","# Epoch 163: val_accuracy did not improve from 0.72034\n","\n","# Epoch 163: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6620 - accuracy: 0.7729 - val_loss: 1.0748 - val_accuracy: 0.6907\n","# Epoch 164/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6680 - accuracy: 0.7745\n","# Epoch 164: val_accuracy did not improve from 0.72034\n","\n","# Epoch 164: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6680 - accuracy: 0.7745 - val_loss: 1.1059 - val_accuracy: 0.6653\n","# Epoch 165/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6545 - accuracy: 0.7750\n","# Epoch 165: val_accuracy did not improve from 0.72034\n","\n","# Epoch 165: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6545 - accuracy: 0.7750 - val_loss: 1.1124 - val_accuracy: 0.6992\n","# Epoch 166/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6583 - accuracy: 0.7721\n","# Epoch 166: val_accuracy did not improve from 0.72034\n","\n","# Epoch 166: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6583 - accuracy: 0.7721 - val_loss: 1.0695 - val_accuracy: 0.6737\n","# Epoch 167/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6446 - accuracy: 0.7817\n","# Epoch 167: val_accuracy did not improve from 0.72034\n","\n","# Epoch 167: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6446 - accuracy: 0.7820 - val_loss: 1.1081 - val_accuracy: 0.6907\n","# Epoch 168/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6759 - accuracy: 0.7709\n","# Epoch 168: val_accuracy did not improve from 0.72034\n","\n","# Epoch 168: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6759 - accuracy: 0.7709 - val_loss: 0.9769 - val_accuracy: 0.7076\n","# Epoch 169/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6492 - accuracy: 0.7727\n","# Epoch 169: val_accuracy improved from 0.72034 to 0.72458, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 169: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 9s 26ms/step - loss: 0.6492 - accuracy: 0.7727 - val_loss: 1.0655 - val_accuracy: 0.7246\n","# Epoch 170/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6490 - accuracy: 0.7730\n","# Epoch 170: val_accuracy did not improve from 0.72458\n","\n","# Epoch 170: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6490 - accuracy: 0.7730 - val_loss: 1.1743 - val_accuracy: 0.6970\n","# Epoch 171/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6557 - accuracy: 0.7770\n","# Epoch 171: val_accuracy did not improve from 0.72458\n","\n","# Epoch 171: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6557 - accuracy: 0.7770 - val_loss: 1.0949 - val_accuracy: 0.7119\n","# Epoch 172/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6516 - accuracy: 0.7724\n","# Epoch 172: val_accuracy did not improve from 0.72458\n","\n","# Epoch 172: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6518 - accuracy: 0.7723 - val_loss: 1.0972 - val_accuracy: 0.7034\n","# Epoch 173/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.6504 - accuracy: 0.7758\n","# Epoch 173: val_accuracy did not improve from 0.72458\n","\n","# Epoch 173: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6521 - accuracy: 0.7754 - val_loss: 1.0423 - val_accuracy: 0.7013\n","# Epoch 174/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6530 - accuracy: 0.7779\n","# Epoch 174: val_accuracy did not improve from 0.72458\n","\n","# Epoch 174: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6519 - accuracy: 0.7786 - val_loss: 0.9718 - val_accuracy: 0.7034\n","# Epoch 175/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6256 - accuracy: 0.7845\n","# Epoch 175: val_accuracy did not improve from 0.72458\n","\n","# Epoch 175: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6256 - accuracy: 0.7845 - val_loss: 0.9523 - val_accuracy: 0.7182\n","# Epoch 176/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6663 - accuracy: 0.7708\n","# Epoch 176: val_accuracy did not improve from 0.72458\n","\n","# Epoch 176: val_loss did not improve from 0.94320\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6681 - accuracy: 0.7704 - val_loss: 1.0124 - val_accuracy: 0.6843\n","# Epoch 177/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6301 - accuracy: 0.7815\n","# Epoch 177: val_accuracy did not improve from 0.72458\n","\n","# Epoch 177: val_loss improved from 0.94320 to 0.93763, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 9s 27ms/step - loss: 0.6298 - accuracy: 0.7818 - val_loss: 0.9376 - val_accuracy: 0.7013\n","# Epoch 178/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6412 - accuracy: 0.7809\n","# Epoch 178: val_accuracy did not improve from 0.72458\n","\n","# Epoch 178: val_loss improved from 0.93763 to 0.92338, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 9s 27ms/step - loss: 0.6412 - accuracy: 0.7809 - val_loss: 0.9234 - val_accuracy: 0.7034\n","# Epoch 179/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6324 - accuracy: 0.7750\n","# Epoch 179: val_accuracy did not improve from 0.72458\n","\n","# Epoch 179: val_loss did not improve from 0.92338\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6324 - accuracy: 0.7750 - val_loss: 1.0934 - val_accuracy: 0.7055\n","# Epoch 180/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6402 - accuracy: 0.7812\n","# Epoch 180: val_accuracy did not improve from 0.72458\n","\n","# Epoch 180: val_loss did not improve from 0.92338\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6396 - accuracy: 0.7812 - val_loss: 0.9411 - val_accuracy: 0.7013\n","# Epoch 181/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6390 - accuracy: 0.7801\n","# Epoch 181: val_accuracy did not improve from 0.72458\n","\n","# Epoch 181: val_loss did not improve from 0.92338\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6380 - accuracy: 0.7805 - val_loss: 0.9755 - val_accuracy: 0.6970\n","# Epoch 182/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6447 - accuracy: 0.7838\n","# Epoch 182: val_accuracy did not improve from 0.72458\n","\n","# Epoch 182: val_loss did not improve from 0.92338\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6454 - accuracy: 0.7834 - val_loss: 0.9620 - val_accuracy: 0.7076\n","# Epoch 183/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6235 - accuracy: 0.7812\n","# Epoch 183: val_accuracy did not improve from 0.72458\n","\n","# Epoch 183: val_loss improved from 0.92338 to 0.86079, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestLoss\n","# 350/350 [==============================] - 9s 27ms/step - loss: 0.6235 - accuracy: 0.7812 - val_loss: 0.8608 - val_accuracy: 0.7225\n","# Epoch 184/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6618 - accuracy: 0.7763\n","# Epoch 184: val_accuracy did not improve from 0.72458\n","\n","# Epoch 184: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6613 - accuracy: 0.7766 - val_loss: 0.9610 - val_accuracy: 0.7055\n","# Epoch 185/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6519 - accuracy: 0.7745\n","# Epoch 185: val_accuracy improved from 0.72458 to 0.74153, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 185: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 9s 27ms/step - loss: 0.6510 - accuracy: 0.7750 - val_loss: 0.8716 - val_accuracy: 0.7415\n","# Epoch 186/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6140 - accuracy: 0.7906\n","# Epoch 186: val_accuracy did not improve from 0.74153\n","\n","# Epoch 186: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6145 - accuracy: 0.7907 - val_loss: 1.0044 - val_accuracy: 0.6992\n","# Epoch 187/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6614 - accuracy: 0.7708\n","# Epoch 187: val_accuracy did not improve from 0.74153\n","\n","# Epoch 187: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6606 - accuracy: 0.7707 - val_loss: 1.0017 - val_accuracy: 0.7097\n","# Epoch 188/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6153 - accuracy: 0.7889\n","# Epoch 188: val_accuracy did not improve from 0.74153\n","\n","# Epoch 188: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6153 - accuracy: 0.7889 - val_loss: 1.1148 - val_accuracy: 0.6716\n","# Epoch 189/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6852 - accuracy: 0.7735\n","# Epoch 189: val_accuracy did not improve from 0.74153\n","\n","# Epoch 189: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6847 - accuracy: 0.7739 - val_loss: 0.9158 - val_accuracy: 0.7161\n","# Epoch 190/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6368 - accuracy: 0.7795\n","# Epoch 190: val_accuracy did not improve from 0.74153\n","\n","# Epoch 190: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6368 - accuracy: 0.7795 - val_loss: 0.9587 - val_accuracy: 0.7055\n","# Epoch 191/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6459 - accuracy: 0.7845\n","# Epoch 191: val_accuracy did not improve from 0.74153\n","\n","# Epoch 191: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6463 - accuracy: 0.7843 - val_loss: 1.0225 - val_accuracy: 0.7119\n","# Epoch 192/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6565 - accuracy: 0.7775\n","# Epoch 192: val_accuracy did not improve from 0.74153\n","\n","# Epoch 192: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6579 - accuracy: 0.7770 - val_loss: 1.1230 - val_accuracy: 0.6695\n","# Epoch 193/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6369 - accuracy: 0.7797\n","# Epoch 193: val_accuracy did not improve from 0.74153\n","\n","# Epoch 193: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6362 - accuracy: 0.7802 - val_loss: 1.0174 - val_accuracy: 0.6907\n","# Epoch 194/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6446 - accuracy: 0.7771\n","# Epoch 194: val_accuracy did not improve from 0.74153\n","\n","# Epoch 194: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6449 - accuracy: 0.7771 - val_loss: 0.9845 - val_accuracy: 0.7119\n","# Epoch 195/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6348 - accuracy: 0.7841\n","# Epoch 195: val_accuracy did not improve from 0.74153\n","\n","# Epoch 195: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6348 - accuracy: 0.7841 - val_loss: 1.0163 - val_accuracy: 0.6928\n","# Epoch 196/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6212 - accuracy: 0.7937\n","# Epoch 196: val_accuracy did not improve from 0.74153\n","\n","# Epoch 196: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6216 - accuracy: 0.7936 - val_loss: 1.1765 - val_accuracy: 0.6674\n","# Epoch 197/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6293 - accuracy: 0.7825\n","# Epoch 197: val_accuracy did not improve from 0.74153\n","\n","# Epoch 197: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6293 - accuracy: 0.7825 - val_loss: 1.1049 - val_accuracy: 0.6610\n","# Epoch 198/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.6459 - accuracy: 0.7767\n","# Epoch 198: val_accuracy did not improve from 0.74153\n","\n","# Epoch 198: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6463 - accuracy: 0.7766 - val_loss: 0.9839 - val_accuracy: 0.7203\n","# Epoch 199/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6218 - accuracy: 0.7834\n","# Epoch 199: val_accuracy did not improve from 0.74153\n","\n","# Epoch 199: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6233 - accuracy: 0.7832 - val_loss: 1.0206 - val_accuracy: 0.6970\n","# Epoch 200/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6143 - accuracy: 0.7929\n","# Epoch 200: val_accuracy did not improve from 0.74153\n","\n","# Epoch 200: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6143 - accuracy: 0.7929 - val_loss: 1.1965 - val_accuracy: 0.6822\n","# Epoch 201/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.6314 - accuracy: 0.7835\n","# Epoch 201: val_accuracy did not improve from 0.74153\n","\n","# Epoch 201: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6305 - accuracy: 0.7841 - val_loss: 0.8915 - val_accuracy: 0.7182\n","# Epoch 202/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6196 - accuracy: 0.7896\n","# Epoch 202: val_accuracy did not improve from 0.74153\n","\n","# Epoch 202: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6205 - accuracy: 0.7898 - val_loss: 1.0914 - val_accuracy: 0.6907\n","# Epoch 203/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6429 - accuracy: 0.7787\n","# Epoch 203: val_accuracy did not improve from 0.74153\n","\n","# Epoch 203: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6431 - accuracy: 0.7784 - val_loss: 0.9349 - val_accuracy: 0.7203\n","# Epoch 204/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6154 - accuracy: 0.7895\n","# Epoch 204: val_accuracy did not improve from 0.74153\n","\n","# Epoch 204: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6155 - accuracy: 0.7896 - val_loss: 0.9873 - val_accuracy: 0.6822\n","# Epoch 205/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6357 - accuracy: 0.7855\n","# Epoch 205: val_accuracy did not improve from 0.74153\n","\n","# Epoch 205: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6353 - accuracy: 0.7854 - val_loss: 0.9539 - val_accuracy: 0.7288\n","# Epoch 206/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6152 - accuracy: 0.7872\n","# Epoch 206: val_accuracy did not improve from 0.74153\n","\n","# Epoch 206: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6169 - accuracy: 0.7871 - val_loss: 0.9405 - val_accuracy: 0.7309\n","# Epoch 207/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6333 - accuracy: 0.7809\n","# Epoch 207: val_accuracy did not improve from 0.74153\n","\n","# Epoch 207: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6333 - accuracy: 0.7809 - val_loss: 0.9896 - val_accuracy: 0.6886\n","# Epoch 208/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6418 - accuracy: 0.7814\n","# Epoch 208: val_accuracy did not improve from 0.74153\n","\n","# Epoch 208: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6419 - accuracy: 0.7818 - val_loss: 0.9851 - val_accuracy: 0.7097\n","# Epoch 209/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6039 - accuracy: 0.7930\n","# Epoch 209: val_accuracy did not improve from 0.74153\n","\n","# Epoch 209: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6034 - accuracy: 0.7930 - val_loss: 1.0005 - val_accuracy: 0.7097\n","# Epoch 210/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.6080 - accuracy: 0.7918\n","# Epoch 210: val_accuracy did not improve from 0.74153\n","\n","# Epoch 210: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6084 - accuracy: 0.7916 - val_loss: 0.9598 - val_accuracy: 0.7140\n","# Epoch 211/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6120 - accuracy: 0.7873\n","# Epoch 211: val_accuracy did not improve from 0.74153\n","\n","# Epoch 211: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6120 - accuracy: 0.7873 - val_loss: 0.9680 - val_accuracy: 0.6949\n","# Epoch 212/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.6298 - accuracy: 0.7824\n","# Epoch 212: val_accuracy did not improve from 0.74153\n","\n","# Epoch 212: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6298 - accuracy: 0.7825 - val_loss: 1.0436 - val_accuracy: 0.6674\n","# Epoch 213/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6332 - accuracy: 0.7844\n","# Epoch 213: val_accuracy did not improve from 0.74153\n","\n","# Epoch 213: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6327 - accuracy: 0.7845 - val_loss: 0.9915 - val_accuracy: 0.7013\n","# Epoch 214/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6172 - accuracy: 0.7929\n","# Epoch 214: val_accuracy did not improve from 0.74153\n","\n","# Epoch 214: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6172 - accuracy: 0.7929 - val_loss: 1.0311 - val_accuracy: 0.7161\n","# Epoch 215/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6384 - accuracy: 0.7859\n","# Epoch 215: val_accuracy did not improve from 0.74153\n","\n","# Epoch 215: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6384 - accuracy: 0.7859 - val_loss: 1.0473 - val_accuracy: 0.7076\n","# Epoch 216/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6213 - accuracy: 0.7886\n","# Epoch 216: val_accuracy did not improve from 0.74153\n","\n","# Epoch 216: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6213 - accuracy: 0.7886 - val_loss: 0.9818 - val_accuracy: 0.7246\n","# Epoch 217/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6010 - accuracy: 0.7937\n","# Epoch 217: val_accuracy did not improve from 0.74153\n","\n","# Epoch 217: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6010 - accuracy: 0.7937 - val_loss: 0.9016 - val_accuracy: 0.7309\n","# Epoch 218/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.5975 - accuracy: 0.7989\n","# Epoch 218: val_accuracy did not improve from 0.74153\n","\n","# Epoch 218: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5966 - accuracy: 0.7989 - val_loss: 1.0999 - val_accuracy: 0.6907\n","# Epoch 219/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6012 - accuracy: 0.7993\n","# Epoch 219: val_accuracy did not improve from 0.74153\n","\n","# Epoch 219: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6012 - accuracy: 0.7993 - val_loss: 1.1321 - val_accuracy: 0.6864\n","# Epoch 220/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6356 - accuracy: 0.7851\n","# Epoch 220: val_accuracy improved from 0.74153 to 0.75000, saving model to /kaggle/working/anotherCopyOfModel29_numClasses7/anotherCopyOfModel29_numClasses7_300Epoch_BestAccuracy\n","\n","# Epoch 220: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 9s 27ms/step - loss: 0.6348 - accuracy: 0.7854 - val_loss: 0.9309 - val_accuracy: 0.7500\n","# Epoch 221/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6028 - accuracy: 0.7923\n","# Epoch 221: val_accuracy did not improve from 0.75000\n","\n","# Epoch 221: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6028 - accuracy: 0.7923 - val_loss: 1.0585 - val_accuracy: 0.6758\n","# Epoch 222/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6152 - accuracy: 0.7891\n","# Epoch 222: val_accuracy did not improve from 0.75000\n","\n","# Epoch 222: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6152 - accuracy: 0.7891 - val_loss: 1.1044 - val_accuracy: 0.6949\n","# Epoch 223/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6067 - accuracy: 0.7935\n","# Epoch 223: val_accuracy did not improve from 0.75000\n","\n","# Epoch 223: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6052 - accuracy: 0.7943 - val_loss: 0.9906 - val_accuracy: 0.7309\n","# Epoch 224/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6335 - accuracy: 0.7799\n","# Epoch 224: val_accuracy did not improve from 0.75000\n","\n","# Epoch 224: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6329 - accuracy: 0.7802 - val_loss: 1.0670 - val_accuracy: 0.6907\n","# Epoch 225/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6191 - accuracy: 0.7892\n","# Epoch 225: val_accuracy did not improve from 0.75000\n","\n","# Epoch 225: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6191 - accuracy: 0.7893 - val_loss: 1.0655 - val_accuracy: 0.6907\n","# Epoch 226/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6773 - accuracy: 0.7761\n","# Epoch 226: val_accuracy did not improve from 0.75000\n","\n","# Epoch 226: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6773 - accuracy: 0.7761 - val_loss: 1.1078 - val_accuracy: 0.6886\n","# Epoch 227/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6175 - accuracy: 0.7904\n","# Epoch 227: val_accuracy did not improve from 0.75000\n","\n","# Epoch 227: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6175 - accuracy: 0.7904 - val_loss: 0.9213 - val_accuracy: 0.7161\n","# Epoch 228/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6195 - accuracy: 0.7915\n","# Epoch 228: val_accuracy did not improve from 0.75000\n","\n","# Epoch 228: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6192 - accuracy: 0.7916 - val_loss: 1.1919 - val_accuracy: 0.7161\n","# Epoch 229/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6346 - accuracy: 0.7839\n","# Epoch 229: val_accuracy did not improve from 0.75000\n","\n","# Epoch 229: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6346 - accuracy: 0.7839 - val_loss: 1.1563 - val_accuracy: 0.6949\n","# Epoch 230/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6366 - accuracy: 0.7893\n","# Epoch 230: val_accuracy did not improve from 0.75000\n","\n","# Epoch 230: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6366 - accuracy: 0.7893 - val_loss: 1.0206 - val_accuracy: 0.6886\n","# Epoch 231/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.6155 - accuracy: 0.7956\n","# Epoch 231: val_accuracy did not improve from 0.75000\n","\n","# Epoch 231: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6165 - accuracy: 0.7952 - val_loss: 0.9252 - val_accuracy: 0.7246\n","# Epoch 232/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6153 - accuracy: 0.7933\n","# Epoch 232: val_accuracy did not improve from 0.75000\n","\n","# Epoch 232: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6179 - accuracy: 0.7923 - val_loss: 1.0949 - val_accuracy: 0.6822\n","# Epoch 233/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.6152 - accuracy: 0.7896\n","# Epoch 233: val_accuracy did not improve from 0.75000\n","\n","# Epoch 233: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6193 - accuracy: 0.7886 - val_loss: 1.0466 - val_accuracy: 0.6801\n","# Epoch 234/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6133 - accuracy: 0.7888\n","# Epoch 234: val_accuracy did not improve from 0.75000\n","\n","# Epoch 234: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6124 - accuracy: 0.7889 - val_loss: 1.1573 - val_accuracy: 0.6928\n","# Epoch 235/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6269 - accuracy: 0.7881\n","# Epoch 235: val_accuracy did not improve from 0.75000\n","\n","# Epoch 235: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6282 - accuracy: 0.7879 - val_loss: 0.9437 - val_accuracy: 0.7034\n","# Epoch 236/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.6238 - accuracy: 0.7905\n","# Epoch 236: val_accuracy did not improve from 0.75000\n","\n","# Epoch 236: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6265 - accuracy: 0.7895 - val_loss: 1.0415 - val_accuracy: 0.6886\n","# Epoch 237/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6075 - accuracy: 0.7952\n","# Epoch 237: val_accuracy did not improve from 0.75000\n","\n","# Epoch 237: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6075 - accuracy: 0.7952 - val_loss: 1.1276 - val_accuracy: 0.6780\n","# Epoch 238/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6253 - accuracy: 0.7857\n","# Epoch 238: val_accuracy did not improve from 0.75000\n","\n","# Epoch 238: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6244 - accuracy: 0.7862 - val_loss: 1.0488 - val_accuracy: 0.6949\n","# Epoch 239/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6173 - accuracy: 0.7884\n","# Epoch 239: val_accuracy did not improve from 0.75000\n","\n","# Epoch 239: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6171 - accuracy: 0.7886 - val_loss: 1.0412 - val_accuracy: 0.7097\n","# Epoch 240/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.5932 - accuracy: 0.7958\n","# Epoch 240: val_accuracy did not improve from 0.75000\n","\n","# Epoch 240: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5923 - accuracy: 0.7961 - val_loss: 1.0385 - val_accuracy: 0.7394\n","# Epoch 241/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.5904 - accuracy: 0.7983\n","# Epoch 241: val_accuracy did not improve from 0.75000\n","\n","# Epoch 241: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5910 - accuracy: 0.7979 - val_loss: 1.0102 - val_accuracy: 0.7119\n","# Epoch 242/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6051 - accuracy: 0.8011\n","# Epoch 242: val_accuracy did not improve from 0.75000\n","\n","# Epoch 242: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6051 - accuracy: 0.8011 - val_loss: 1.0290 - val_accuracy: 0.7373\n","# Epoch 243/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6206 - accuracy: 0.7936\n","# Epoch 243: val_accuracy did not improve from 0.75000\n","\n","# Epoch 243: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6201 - accuracy: 0.7934 - val_loss: 1.0137 - val_accuracy: 0.7097\n","# Epoch 244/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6199 - accuracy: 0.7902\n","# Epoch 244: val_accuracy did not improve from 0.75000\n","\n","# Epoch 244: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6199 - accuracy: 0.7902 - val_loss: 0.9537 - val_accuracy: 0.7076\n","# Epoch 245/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6165 - accuracy: 0.7870\n","# Epoch 245: val_accuracy did not improve from 0.75000\n","\n","# Epoch 245: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6159 - accuracy: 0.7871 - val_loss: 0.9585 - val_accuracy: 0.7394\n","# Epoch 246/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6154 - accuracy: 0.7898\n","# Epoch 246: val_accuracy did not improve from 0.75000\n","\n","# Epoch 246: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6154 - accuracy: 0.7898 - val_loss: 1.1898 - val_accuracy: 0.6992\n","# Epoch 247/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6069 - accuracy: 0.7948\n","# Epoch 247: val_accuracy did not improve from 0.75000\n","\n","# Epoch 247: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6071 - accuracy: 0.7950 - val_loss: 0.9729 - val_accuracy: 0.7225\n","# Epoch 248/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6117 - accuracy: 0.7908\n","# Epoch 248: val_accuracy did not improve from 0.75000\n","\n","# Epoch 248: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6107 - accuracy: 0.7911 - val_loss: 0.9333 - val_accuracy: 0.7415\n","# Epoch 249/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6264 - accuracy: 0.7912\n","# Epoch 249: val_accuracy did not improve from 0.75000\n","\n","# Epoch 249: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6258 - accuracy: 0.7912 - val_loss: 1.0472 - val_accuracy: 0.7055\n","# Epoch 250/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6319 - accuracy: 0.7900\n","# Epoch 250: val_accuracy did not improve from 0.75000\n","\n","# Epoch 250: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6319 - accuracy: 0.7900 - val_loss: 1.0841 - val_accuracy: 0.6886\n","# Epoch 251/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6044 - accuracy: 0.7941\n","# Epoch 251: val_accuracy did not improve from 0.75000\n","\n","# Epoch 251: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6044 - accuracy: 0.7941 - val_loss: 1.0057 - val_accuracy: 0.7309\n","# Epoch 252/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6179 - accuracy: 0.7889\n","# Epoch 252: val_accuracy did not improve from 0.75000\n","\n","# Epoch 252: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6179 - accuracy: 0.7889 - val_loss: 1.1435 - val_accuracy: 0.6801\n","# Epoch 253/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6367 - accuracy: 0.7878\n","# Epoch 253: val_accuracy did not improve from 0.75000\n","\n","# Epoch 253: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6362 - accuracy: 0.7877 - val_loss: 1.2434 - val_accuracy: 0.6801\n","# Epoch 254/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6169 - accuracy: 0.7979\n","# Epoch 254: val_accuracy did not improve from 0.75000\n","\n","# Epoch 254: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6169 - accuracy: 0.7979 - val_loss: 1.0479 - val_accuracy: 0.6843\n","# Epoch 255/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6017 - accuracy: 0.7972\n","# Epoch 255: val_accuracy did not improve from 0.75000\n","\n","# Epoch 255: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6026 - accuracy: 0.7970 - val_loss: 1.0247 - val_accuracy: 0.6801\n","# Epoch 256/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.5894 - accuracy: 0.8023\n","# Epoch 256: val_accuracy did not improve from 0.75000\n","\n","# Epoch 256: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5885 - accuracy: 0.8025 - val_loss: 1.1565 - val_accuracy: 0.6780\n","# Epoch 257/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.5787 - accuracy: 0.8061\n","# Epoch 257: val_accuracy did not improve from 0.75000\n","\n","# Epoch 257: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.5787 - accuracy: 0.8061 - val_loss: 1.0979 - val_accuracy: 0.6886\n","# Epoch 258/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6095 - accuracy: 0.7948\n","# Epoch 258: val_accuracy did not improve from 0.75000\n","\n","# Epoch 258: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6094 - accuracy: 0.7948 - val_loss: 1.0001 - val_accuracy: 0.7076\n","# Epoch 259/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6006 - accuracy: 0.7975\n","# Epoch 259: val_accuracy did not improve from 0.75000\n","\n","# Epoch 259: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6006 - accuracy: 0.7975 - val_loss: 0.9501 - val_accuracy: 0.7246\n","# Epoch 260/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6023 - accuracy: 0.7957\n","# Epoch 260: val_accuracy did not improve from 0.75000\n","\n","# Epoch 260: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6019 - accuracy: 0.7961 - val_loss: 1.0883 - val_accuracy: 0.6949\n","# Epoch 261/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6055 - accuracy: 0.8014\n","# Epoch 261: val_accuracy did not improve from 0.75000\n","\n","# Epoch 261: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6054 - accuracy: 0.8012 - val_loss: 1.0161 - val_accuracy: 0.7119\n","# Epoch 262/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6228 - accuracy: 0.7926\n","# Epoch 262: val_accuracy did not improve from 0.75000\n","\n","# Epoch 262: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6222 - accuracy: 0.7925 - val_loss: 0.9926 - val_accuracy: 0.7076\n","# Epoch 263/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6146 - accuracy: 0.7895\n","# Epoch 263: val_accuracy did not improve from 0.75000\n","\n","# Epoch 263: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6160 - accuracy: 0.7893 - val_loss: 1.0588 - val_accuracy: 0.7161\n","# Epoch 264/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.5980 - accuracy: 0.8030\n","# Epoch 264: val_accuracy did not improve from 0.75000\n","\n","# Epoch 264: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5980 - accuracy: 0.8030 - val_loss: 0.9621 - val_accuracy: 0.6949\n","# Epoch 265/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.5881 - accuracy: 0.7989\n","# Epoch 265: val_accuracy did not improve from 0.75000\n","\n","# Epoch 265: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.5881 - accuracy: 0.7989 - val_loss: 0.9659 - val_accuracy: 0.7140\n","# Epoch 266/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.5902 - accuracy: 0.7946\n","# Epoch 266: val_accuracy did not improve from 0.75000\n","\n","# Epoch 266: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.5902 - accuracy: 0.7946 - val_loss: 1.0872 - val_accuracy: 0.7097\n","# Epoch 267/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.5831 - accuracy: 0.8043\n","# Epoch 267: val_accuracy did not improve from 0.75000\n","\n","# Epoch 267: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5831 - accuracy: 0.8043 - val_loss: 1.0186 - val_accuracy: 0.6992\n","# Epoch 268/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6045 - accuracy: 0.7985\n","# Epoch 268: val_accuracy did not improve from 0.75000\n","\n","# Epoch 268: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6046 - accuracy: 0.7987 - val_loss: 1.0317 - val_accuracy: 0.7331\n","# Epoch 269/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.5825 - accuracy: 0.8050\n","# Epoch 269: val_accuracy did not improve from 0.75000\n","\n","# Epoch 269: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5825 - accuracy: 0.8050 - val_loss: 0.9505 - val_accuracy: 0.7246\n","# Epoch 270/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.5788 - accuracy: 0.8039\n","# Epoch 270: val_accuracy did not improve from 0.75000\n","\n","# Epoch 270: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5781 - accuracy: 0.8041 - val_loss: 1.0120 - val_accuracy: 0.7140\n","# Epoch 271/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6167 - accuracy: 0.7886\n","# Epoch 271: val_accuracy did not improve from 0.75000\n","\n","# Epoch 271: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6170 - accuracy: 0.7887 - val_loss: 0.9956 - val_accuracy: 0.7055\n","# Epoch 272/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6090 - accuracy: 0.7937\n","# Epoch 272: val_accuracy did not improve from 0.75000\n","\n","# Epoch 272: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6084 - accuracy: 0.7939 - val_loss: 1.1666 - val_accuracy: 0.6589\n","# Epoch 273/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.5956 - accuracy: 0.7992\n","# Epoch 273: val_accuracy did not improve from 0.75000\n","\n","# Epoch 273: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5955 - accuracy: 0.7991 - val_loss: 1.1213 - val_accuracy: 0.6992\n","# Epoch 274/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6121 - accuracy: 0.7959\n","# Epoch 274: val_accuracy did not improve from 0.75000\n","\n","# Epoch 274: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6121 - accuracy: 0.7959 - val_loss: 0.9022 - val_accuracy: 0.7225\n","# Epoch 275/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.5762 - accuracy: 0.8046\n","# Epoch 275: val_accuracy did not improve from 0.75000\n","\n","# Epoch 275: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5762 - accuracy: 0.8046 - val_loss: 1.0907 - val_accuracy: 0.6949\n","# Epoch 276/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.6038 - accuracy: 0.7974\n","# Epoch 276: val_accuracy did not improve from 0.75000\n","\n","# Epoch 276: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6026 - accuracy: 0.7977 - val_loss: 1.0918 - val_accuracy: 0.6949\n","# Epoch 277/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.5919 - accuracy: 0.8061\n","# Epoch 277: val_accuracy did not improve from 0.75000\n","\n","# Epoch 277: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.5919 - accuracy: 0.8061 - val_loss: 0.9357 - val_accuracy: 0.7246\n","# Epoch 278/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6221 - accuracy: 0.7884\n","# Epoch 278: val_accuracy did not improve from 0.75000\n","\n","# Epoch 278: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6221 - accuracy: 0.7884 - val_loss: 1.1394 - val_accuracy: 0.6822\n","# Epoch 279/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.5882 - accuracy: 0.8045\n","# Epoch 279: val_accuracy did not improve from 0.75000\n","\n","# Epoch 279: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5882 - accuracy: 0.8045 - val_loss: 1.0054 - val_accuracy: 0.7013\n","# Epoch 280/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.5800 - accuracy: 0.7947\n","# Epoch 280: val_accuracy did not improve from 0.75000\n","\n","# Epoch 280: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5795 - accuracy: 0.7950 - val_loss: 1.0463 - val_accuracy: 0.7394\n","# Epoch 281/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.5897 - accuracy: 0.7961\n","# Epoch 281: val_accuracy did not improve from 0.75000\n","\n","# Epoch 281: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5897 - accuracy: 0.7961 - val_loss: 0.9984 - val_accuracy: 0.7436\n","# Epoch 282/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.5933 - accuracy: 0.8007\n","# Epoch 282: val_accuracy did not improve from 0.75000\n","\n","# Epoch 282: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.5935 - accuracy: 0.8007 - val_loss: 1.0050 - val_accuracy: 0.7182\n","# Epoch 283/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.5849 - accuracy: 0.8042\n","# Epoch 283: val_accuracy did not improve from 0.75000\n","\n","# Epoch 283: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5863 - accuracy: 0.8039 - val_loss: 1.0012 - val_accuracy: 0.6992\n","# Epoch 284/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.5824 - accuracy: 0.8048\n","# Epoch 284: val_accuracy did not improve from 0.75000\n","\n","# Epoch 284: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5823 - accuracy: 0.8048 - val_loss: 0.9413 - val_accuracy: 0.7182\n","# Epoch 285/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6002 - accuracy: 0.8018\n","# Epoch 285: val_accuracy did not improve from 0.75000\n","\n","# Epoch 285: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5994 - accuracy: 0.8020 - val_loss: 0.8989 - val_accuracy: 0.7140\n","# Epoch 286/300\n","# 347/350 [============================>.] - ETA: 0s - loss: 0.5978 - accuracy: 0.8037\n","# Epoch 286: val_accuracy did not improve from 0.75000\n","\n","# Epoch 286: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5955 - accuracy: 0.8041 - val_loss: 1.0142 - val_accuracy: 0.7076\n","# Epoch 287/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.7381 - accuracy: 0.7477\n","# Epoch 287: val_accuracy did not improve from 0.75000\n","\n","# Epoch 287: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.7381 - accuracy: 0.7477 - val_loss: 1.0033 - val_accuracy: 0.7225\n","# Epoch 288/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.6465 - accuracy: 0.7816\n","# Epoch 288: val_accuracy did not improve from 0.75000\n","\n","# Epoch 288: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6465 - accuracy: 0.7816 - val_loss: 1.1848 - val_accuracy: 0.6928\n","# Epoch 289/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.5941 - accuracy: 0.7982\n","# Epoch 289: val_accuracy did not improve from 0.75000\n","\n","# Epoch 289: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5935 - accuracy: 0.7982 - val_loss: 1.0153 - val_accuracy: 0.7203\n","# Epoch 290/300\n","# 350/350 [==============================] - ETA: 0s - loss: 0.5873 - accuracy: 0.8029\n","# Epoch 290: val_accuracy did not improve from 0.75000\n","\n","# Epoch 290: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5873 - accuracy: 0.8029 - val_loss: 1.0470 - val_accuracy: 0.7161\n","# Epoch 291/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.5889 - accuracy: 0.8032\n","# Epoch 291: val_accuracy did not improve from 0.75000\n","\n","# Epoch 291: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5882 - accuracy: 0.8032 - val_loss: 1.0462 - val_accuracy: 0.7034\n","# Epoch 292/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6037 - accuracy: 0.7963\n","# Epoch 292: val_accuracy did not improve from 0.75000\n","\n","# Epoch 292: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6041 - accuracy: 0.7961 - val_loss: 1.0546 - val_accuracy: 0.7140\n","# Epoch 293/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6278 - accuracy: 0.7827\n","# Epoch 293: val_accuracy did not improve from 0.75000\n","\n","# Epoch 293: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6270 - accuracy: 0.7829 - val_loss: 1.0597 - val_accuracy: 0.7055\n","# Epoch 294/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.5986 - accuracy: 0.7957\n","# Epoch 294: val_accuracy did not improve from 0.75000\n","\n","# Epoch 294: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5980 - accuracy: 0.7955 - val_loss: 1.3622 - val_accuracy: 0.6547\n","# Epoch 295/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6432 - accuracy: 0.7911\n","# Epoch 295: val_accuracy did not improve from 0.75000\n","\n","# Epoch 295: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6426 - accuracy: 0.7911 - val_loss: 1.0533 - val_accuracy: 0.7161\n","# Epoch 296/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.5864 - accuracy: 0.7953\n","# Epoch 296: val_accuracy did not improve from 0.75000\n","\n","# Epoch 296: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5871 - accuracy: 0.7950 - val_loss: 1.0558 - val_accuracy: 0.7034\n","# Epoch 297/300\n","# 348/350 [============================>.] - ETA: 0s - loss: 0.6226 - accuracy: 0.7938\n","# Epoch 297: val_accuracy did not improve from 0.75000\n","\n","# Epoch 297: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 18ms/step - loss: 0.6217 - accuracy: 0.7937 - val_loss: 0.9960 - val_accuracy: 0.7331\n","# Epoch 298/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.6052 - accuracy: 0.7932\n","# Epoch 298: val_accuracy did not improve from 0.75000\n","\n","# Epoch 298: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.6055 - accuracy: 0.7932 - val_loss: 1.0892 - val_accuracy: 0.6864\n","# Epoch 299/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.5839 - accuracy: 0.8018\n","# Epoch 299: val_accuracy did not improve from 0.75000\n","\n","# Epoch 299: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5827 - accuracy: 0.8021 - val_loss: 1.0957 - val_accuracy: 0.6949\n","# Epoch 300/300\n","# 349/350 [============================>.] - ETA: 0s - loss: 0.5815 - accuracy: 0.8012\n","# Epoch 300: val_accuracy did not improve from 0.75000\n","\n","# Epoch 300: val_loss did not improve from 0.86079\n","# 350/350 [==============================] - 6s 17ms/step - loss: 0.5811 - accuracy: 0.8016 - val_loss: 1.0672 - val_accuracy: 0.7119"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4111391,"sourceId":7126960,"sourceType":"datasetVersion"}],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
